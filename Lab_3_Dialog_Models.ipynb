{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 3 - Dialog Models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZWmXHDbeXQM0",
        "Q-T6U0SwXug1",
        "FYZvlg22YRJl",
        "jq5OEAviX6gM",
        "bMYXTUxTYqda",
        "hMbqhVAmY0Se",
        "Dx9nbg0GYg2I",
        "bZDb9R1PYlQb",
        "O8I1I9d-ZYtN",
        "Toy0CafuZoQl",
        "hed9HtI0uJKE",
        "bq0710F3uj_t",
        "_3QGfSWyb7cu",
        "ruWvJHkqfVog",
        "Wlc49nXVwdDf",
        "UKd426dMw8kL",
        "9rsy_MyyH0nN",
        "_7505DPQnKp4",
        "XWIdI0gwmy5d",
        "_HSHgSB0rB4Q",
        "9dOxj3ODrcn0",
        "4otMF2F1_yRX",
        "n1zAdsGp0jKn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4dceba7b19ea446399dd5e3173f4c76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee89208102134e76a586263c73e81872",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61b39a0b4b2046d5a40c108a720fc39f",
              "IPY_MODEL_546e4e91c4934344b9cd8d89edb3536c"
            ]
          }
        },
        "ee89208102134e76a586263c73e81872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61b39a0b4b2046d5a40c108a720fc39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad9fdf36e2284701a42eabea88fc64e4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56064ac5495749509225eb957db4112a"
          }
        },
        "546e4e91c4934344b9cd8d89edb3536c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4351a713794a44de9e984270560f4c43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:24&lt;00:00, 42.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f3214bbef6c43059c8f8ff797524715"
          }
        },
        "ad9fdf36e2284701a42eabea88fc64e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56064ac5495749509225eb957db4112a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4351a713794a44de9e984270560f4c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f3214bbef6c43059c8f8ff797524715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a55c90f17914093be405fb2b784ce40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e6f529563f343ca8f8dc21c523823f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d1ede52900941fe983db6e403a1cfd8",
              "IPY_MODEL_3895dabfb7d04403a087e70e571e70d3"
            ]
          }
        },
        "6e6f529563f343ca8f8dc21c523823f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d1ede52900941fe983db6e403a1cfd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7f994f8d3f3d4c7a93f2efb897eb8953",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bfcad86da7147559980391433a3207c"
          }
        },
        "3895dabfb7d04403a087e70e571e70d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93471fb2a24444f0ac856a3dd7988c25",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 965kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f2a2a00fc5543c9be86c69ad7183bc3"
          }
        },
        "7f994f8d3f3d4c7a93f2efb897eb8953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bfcad86da7147559980391433a3207c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93471fb2a24444f0ac856a3dd7988c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f2a2a00fc5543c9be86c69ad7183bc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6767f25b7024527a41726b6cc2d6310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_efec208ca3294b409f267bb369c2baf6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e9597de56e34446e91806dcabfe8b925",
              "IPY_MODEL_7add849b68b34208b79c09cecf0f2b0d"
            ]
          }
        },
        "efec208ca3294b409f267bb369c2baf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9597de56e34446e91806dcabfe8b925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9cffc9cec138409abef31860dc3eca2f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39569b10070a4a3b8bf4e7e289fed375"
          }
        },
        "7add849b68b34208b79c09cecf0f2b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67f94530bd1c48f2bd46e1688ea9217d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 1.78kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_343d12929c0c41dab38cc5a8278e1241"
          }
        },
        "9cffc9cec138409abef31860dc3eca2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39569b10070a4a3b8bf4e7e289fed375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67f94530bd1c48f2bd46e1688ea9217d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "343d12929c0c41dab38cc5a8278e1241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "def2618a93234c7a92a5d83cf195c6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89e29c5111014b3da0e2e55d6703271a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ea088a9b6314aff95d369fc05d64d67",
              "IPY_MODEL_4cf01b2b906d483e9bb6c12a46fa9c6c"
            ]
          }
        },
        "89e29c5111014b3da0e2e55d6703271a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ea088a9b6314aff95d369fc05d64d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b647a90df1254b88ba92c54d4f6ecb2c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_353321b5f36a4042ab371c13d39d9cb6"
          }
        },
        "4cf01b2b906d483e9bb6c12a46fa9c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1ea13eb17ec40a58d64e8ec760b1846",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:21&lt;00:00, 25.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6fe3beec9cf349b1ad42b9b59a43e954"
          }
        },
        "b647a90df1254b88ba92c54d4f6ecb2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "353321b5f36a4042ab371c13d39d9cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1ea13eb17ec40a58d64e8ec760b1846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6fe3beec9cf349b1ad42b9b59a43e954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e1975ffa8e549e8bbe8fafd8cbd3719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a6d5588e89ec4ab98a6a92b9194dd265",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dcaa9c5b9c104db2a73065da5030a618",
              "IPY_MODEL_9b6f2260b1d041588d990aff5f1dcfb3"
            ]
          }
        },
        "a6d5588e89ec4ab98a6a92b9194dd265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcaa9c5b9c104db2a73065da5030a618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec2446f0932b430fae819100705633c4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 807,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 807,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d02a16668b6948479211fb0ebab84a09"
          }
        },
        "9b6f2260b1d041588d990aff5f1dcfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f42ed2d2312c478781cfc3fbb16dd175",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 807/807 [00:12&lt;00:00, 65.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7059b2884704a1a834e1a5ed0e8fe09"
          }
        },
        "ec2446f0932b430fae819100705633c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d02a16668b6948479211fb0ebab84a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f42ed2d2312c478781cfc3fbb16dd175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7059b2884704a1a834e1a5ed0e8fe09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dc4c94626974c8c86f6ec7f9226ca72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32461ab9bc0d4aaab7471eb68d3a1bc4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66aeffb8a7a64448b56d809a0c713421",
              "IPY_MODEL_ee7e7046f1914a6ab0f4a7fc81ecc61b"
            ]
          }
        },
        "32461ab9bc0d4aaab7471eb68d3a1bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66aeffb8a7a64448b56d809a0c713421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_618b03273e77458294c0851df3a5bd33",
            "_dom_classes": [],
            "description": "Epoch 1 of 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1825156af1904e0390aaa006e9891737"
          }
        },
        "ee7e7046f1914a6ab0f4a7fc81ecc61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ab9d7159b5d4fd0ba1996009f6cb4f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [05:52&lt;00:00, 352.82s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04e46c47507f4cfc9d574f9df22d66d7"
          }
        },
        "618b03273e77458294c0851df3a5bd33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1825156af1904e0390aaa006e9891737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ab9d7159b5d4fd0ba1996009f6cb4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04e46c47507f4cfc9d574f9df22d66d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ba4d5fc2c614525bce41a79190e2bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c22bc7045dd84d77a33bd7310af1e02e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c84d74e78644e1db46af679c8e25261",
              "IPY_MODEL_25f1a5b0d88346ceb3113f3b41cfc60c"
            ]
          }
        },
        "c22bc7045dd84d77a33bd7310af1e02e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c84d74e78644e1db46af679c8e25261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a1da0be38b784e0cb03e4a6215fa515a",
            "_dom_classes": [],
            "description": "Running Epoch 0 of 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1816,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1816,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96e9d129d96743a58b211746598d140f"
          }
        },
        "25f1a5b0d88346ceb3113f3b41cfc60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c787f47fcf444a3ca10f52e5ccef37ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1816/1816 [05:52&lt;00:00,  5.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acfaa86d3e054f3488e29fd0ba38fdc7"
          }
        },
        "a1da0be38b784e0cb03e4a6215fa515a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96e9d129d96743a58b211746598d140f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c787f47fcf444a3ca10f52e5ccef37ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acfaa86d3e054f3488e29fd0ba38fdc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmibanthia/2.0.0/blob/master/Lab_3_Dialog_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz9sIB7EWuGL"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> <a href='https://www.computefest.seas.harvard.edu/' target='_blank'><strong>IACS: ComputeFest 2021</strong></a></h1>\n",
        "\n",
        "# **Lab 3 - Dialog Models**\n",
        "\n",
        "#### **Authors/Instructors:**\n",
        "Chris Tanner, Shivas Jayaram, Eduardo Peynetti, Rohit Beri\n",
        "\n",
        "checking changes in github"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_oNq_XEW8Se"
      },
      "source": [
        "## **Workshop Outline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pat2k2WKXQDc"
      },
      "source": [
        "Overview Dataset\n",
        "\n",
        "Dialog task using GPT2 Model\n",
        "\n",
        "- Language generation without finetuning\n",
        "- Language generation with finetuning\n",
        "- Nano Quiz\n",
        "\n",
        "Dialog task using GPT2 Double Head Model\n",
        "\n",
        "- What is a GPT2 Double Head Model?\n",
        "- Dialogs using a finetuned but on different dataset\n",
        "- Finetuning an already fintuned model to our dataset\n",
        "- Nano Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWmXHDbeXQM0"
      },
      "source": [
        "## **Setup Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5YwitxhXY7Z"
      },
      "source": [
        "#### Copy & setup Colab with GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTvPWx3FXnhG"
      },
      "source": [
        "1) Select \"File\" menu and pick \"Save a copy in Drive\"  \n",
        "2) This notebooks is already setup to use GPU but if you want to change it. Go to \"Runtime\" menu and select \"Change runtime type\". Then in the popup in \"Hardware accelerator\" select \"GPU\" and then click \"Save\"   \n",
        "3) If you want high RAM there is an option for that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-T6U0SwXug1"
      },
      "source": [
        "#### Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1gRM_dKVt1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fae5ce-787e-4491-a47f-33f30972b9a3"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.3.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 351kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 59.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.1 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 57.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=6c2a29821f2a8b3dd05275e7871690b45df087c71abbcf356f913237e128bd11\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYZvlg22YRJl"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHGROZdZYTQl"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from argparse import ArgumentParser\n",
        "from subprocess import call\n",
        "import textwrap\n",
        "\n",
        "from collections import defaultdict\n",
        "from multiprocessing import Pool\n",
        "from tqdm.auto import tqdm, trange\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda import amp\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2DoubleHeadsModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5OEAviX6gM"
      },
      "source": [
        "#### Setup Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgaLWUyoX68Y"
      },
      "source": [
        "# Setup Logger\n",
        "if '__file__' not in globals():\n",
        "  __file__ = \".\"\n",
        "logger = logging.getLogger(__file__)\n",
        "\n",
        "# Logger config\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMYXTUxTYqda"
      },
      "source": [
        "#### Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkc7l_wmYq95",
        "outputId": "b5ffc057-4706-46bf-9a23-e81c17411503"
      },
      "source": [
        "logger.info('__Python VERSION: %s', sys.version)\n",
        "logger.info(\"torch version: %s\", torch.__version__)\n",
        "logger.info('CUDNN VERSION: %s', torch.backends.cudnn.version())\n",
        "logger.info('Number CUDA Devices: %s', torch.cuda.device_count())\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
        "device_count = 0\n",
        "\n",
        "if cuda_available:\n",
        "  device_count = torch.cuda.device_count()\n",
        "  logger.info('Devices:')\n",
        "  logger.info('Active CUDA Device: %s', torch.cuda.current_device())\n",
        "  logger.info('Available device count: %s', device_count)\n",
        "  logger.info('Current cuda device: %s', torch.cuda.current_device())\n",
        "else:\n",
        "  logger.info('No CUDA Devices are available')\n",
        "\n",
        "logger.info('Device: %s', device)\n",
        "  \n",
        "\n",
        "# nvidia-smi\n",
        "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:.:__Python VERSION: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "INFO:.:torch version: 1.7.0+cu101\n",
            "INFO:.:CUDNN VERSION: 7603\n",
            "INFO:.:Number CUDA Devices: 1\n",
            "INFO:.:Devices:\n",
            "INFO:.:Active CUDA Device: 0\n",
            "INFO:.:Available device count: 1\n",
            "INFO:.:Current cuda device: 0\n",
            "INFO:.:Device: cuda:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMbqhVAmY0Se"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8h7rBETY1WP"
      },
      "source": [
        "def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n",
        "  if base_path != \"\":\n",
        "    if not os.path.exists(base_path):\n",
        "      os.mkdir(base_path)\n",
        "  packet_file = os.path.basename(packet_url)\n",
        "  with requests.get(packet_url, stream=True, headers=headers) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "  \n",
        "  if extract:\n",
        "    if packet_file.endswith(\".zip\"):\n",
        "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
        "        zfile.extractall(base_path)\n",
        "    else:\n",
        "      packet_name = packet_file.split('.')[0]\n",
        "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
        "        tfile.extractall(base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx9nbg0GYg2I"
      },
      "source": [
        "## **Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZDb9R1PYlQb"
      },
      "source": [
        "#### Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI9qxtunYloP",
        "outputId": "f1712fb0-1c91-4f41-e0f3-864337fced41"
      },
      "source": [
        "start_time = time.time()\n",
        "download_file(\"https://storage.googleapis.com/computefest-2021/dog_data.zip\", base_path=\"datasets\", extract=True)\n",
        "download_file(\"https://storage.googleapis.com/computefest-2021/dogs_qa.csv\", base_path=\"datasets\", extract=False)\n",
        "download_file(\"https://storage.googleapis.com/computefest-2021/personadogchat03.json\", base_path=\"datasets\", extract=False)\n",
        "download_file(\"https://storage.googleapis.com/computefest-2021/bad_words.csv\", base_path=\"datasets\", extract=False)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Download execution time (mins)\",execution_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download execution time (mins) 0.03277856111526489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP3J8UvZV2v"
      },
      "source": [
        "#### Explore Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8I1I9d-ZYtN"
      },
      "source": [
        "##### data_dictionary.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym4F7wWgZarm",
        "outputId": "00a75b8e-58e4-4284-86e8-24181767345c"
      },
      "source": [
        "datasets_path = \"datasets\"\n",
        "\n",
        "# Data Dictionary\n",
        "data_dictionary_path = os.path.join(datasets_path,\"dog_data\",\"data_dictionary.txt\")\n",
        "with open(data_dictionary_path, 'r') as file:\n",
        "  data_dictionary = file.read()\n",
        "\n",
        "print(\"Data Dictionary:\")\n",
        "print(data_dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Dictionary:\n",
            "dogs.csv - one row for every dog taken into custody since 1/1/2017.\n",
            "-fields:\n",
            "—-\"AnimalID\" - public facing unique id\n",
            "--\"AnimalInternal-ID\" - internal unique id - USE THIS to link to the other tables (dogs_photos.csv and dogs_website_memos.csv)\n",
            "--\"AnimalName\" \n",
            "--\"AnimalType\" - always \"Dog\"\n",
            "--\"AnimalSex\" - Male, Female or Unknown\n",
            "--\"AnimalCurrentWeightPounds\" - decimal weight in pounds. NOTE: data quality of this field is mediocre at best. Staff are good about recording at least one weight around the time of intake but not as diligent about recording a weight prior to outcome.\n",
            "--\"AnimalDOB\" -  DOB formatted as YYYYMMDD\n",
            "--\"AnimalBreed\" - concatenation of primary and secondary breed fields delimited by \" /\". \n",
            "--\"AnimalColor\" - concatenation of primary and secondary colors fields delimited by \" /\". \n",
            "--\"AnimalPattern\" - animal pattern NOTE: not often populated for dogs. More often used for cats\n",
            "\n",
            "\n",
            "dogs_photos.csv - one row for every photo uploaded to a dogs profile.\n",
            "-fields:\n",
            "--\"AnimalInternal-ID\" - internal unique id for dog. USE THIS to link to dogs.csv\n",
            "--\"PhotoUrl\" - URL to photo\n",
            "\n",
            "dogs_website_memos.csv - one row for every website bio\n",
            "-fields:\n",
            "--\"AnimalInternal-ID\" - internal unique id for dog. USE THIS to link to dogs.csv\n",
            "--\"MemoText\" - contains public bio text that displays for each animal on their adoption page.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnzvIFPZhGj"
      },
      "source": [
        "##### dogs.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "5aLCHz-2ZhuK",
        "outputId": "f2ae886d-0be5-4de0-c2f2-802718941d2a"
      },
      "source": [
        "# dogs.csv\n",
        "dogs_path = os.path.join(datasets_path,\"dog_data\",\"dogs.csv\")\n",
        "dogs = pd.read_csv(dogs_path)\n",
        "\n",
        "# Compute age of dog\n",
        "dogs['DOB'] = pd.to_datetime(dogs['AnimalDOB'], format='%Y%m%d')\n",
        "dogs[\"Year\"] = pd.DatetimeIndex(dogs['DOB']).year\n",
        "dogs[\"Age\"] = (pd.to_datetime('now') - dogs['DOB']).astype('<m8[Y]')\n",
        "\n",
        "print(\"Shape:\",dogs.shape)\n",
        "dogs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (17212, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AnimalID</th>\n",
              "      <th>AnimalInternal-ID</th>\n",
              "      <th>AnimalName</th>\n",
              "      <th>AnimalType</th>\n",
              "      <th>AnimalSex</th>\n",
              "      <th>AnimalCurrentWeightPounds</th>\n",
              "      <th>AnimalDOB</th>\n",
              "      <th>AnimalBreed</th>\n",
              "      <th>AnimalColor</th>\n",
              "      <th>AnimalPattern</th>\n",
              "      <th>DOB</th>\n",
              "      <th>Year</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45628</td>\n",
              "      <td>1444011</td>\n",
              "      <td>Emma</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Female</td>\n",
              "      <td>53.3</td>\n",
              "      <td>20150306</td>\n",
              "      <td>Retriever, Yellow Labrador /Mix</td>\n",
              "      <td>Blond /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-03-06</td>\n",
              "      <td>2015</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45629</td>\n",
              "      <td>1444014</td>\n",
              "      <td>Rizzoli</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Female</td>\n",
              "      <td>4.7</td>\n",
              "      <td>20161222</td>\n",
              "      <td>Mixed Breed (Small)</td>\n",
              "      <td>Tan /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-12-22</td>\n",
              "      <td>2016</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45630</td>\n",
              "      <td>1444017</td>\n",
              "      <td>Isles</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Female</td>\n",
              "      <td>3.1</td>\n",
              "      <td>20161222</td>\n",
              "      <td>Mixed Breed (Small)</td>\n",
              "      <td>White /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-12-22</td>\n",
              "      <td>2016</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45631</td>\n",
              "      <td>1444020</td>\n",
              "      <td>Cory</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Male</td>\n",
              "      <td>4.7</td>\n",
              "      <td>20161222</td>\n",
              "      <td>Mixed Breed (Small)</td>\n",
              "      <td>Sable /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-12-22</td>\n",
              "      <td>2016</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45632</td>\n",
              "      <td>1444023</td>\n",
              "      <td>Topanga</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Female</td>\n",
              "      <td>8.0</td>\n",
              "      <td>20161222</td>\n",
              "      <td>Mixed Breed (Small)</td>\n",
              "      <td>Tan /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-12-22</td>\n",
              "      <td>2016</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   AnimalID  AnimalInternal-ID AnimalName  ...        DOB  Year  Age\n",
              "0     45628            1444011       Emma  ... 2015-03-06  2015  5.0\n",
              "1     45629            1444014    Rizzoli  ... 2016-12-22  2016  4.0\n",
              "2     45630            1444017      Isles  ... 2016-12-22  2016  4.0\n",
              "3     45631            1444020       Cory  ... 2016-12-22  2016  4.0\n",
              "4     45632            1444023    Topanga  ... 2016-12-22  2016  4.0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toy0CafuZoQl"
      },
      "source": [
        "##### dogs_website_memos.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chu4dE6TZoo4",
        "outputId": "ece0a779-4a13-45ea-d448-635c51fd0e79"
      },
      "source": [
        "# dogs_website_memos.csv\n",
        "dogs_website_memos_path = os.path.join(datasets_path,\"dog_data\",\"dogs_website_memos.csv\")\n",
        "with open(dogs_website_memos_path, 'r') as file:\n",
        "  dogs_website_memos = file.read()\n",
        "\n",
        "dogs_website_memos = dogs_website_memos.replace('\\n\\n','')\n",
        "dogs_website_memos = dogs_website_memos.replace('\\n \\n','')\n",
        "dogs_website_memos = dogs_website_memos.replace('\"\\n','\"<EOL>')\n",
        "dogs_website_memos = dogs_website_memos.replace('\\\\\"','')\n",
        "dogs_website_memos = dogs_website_memos.replace('\\n','')\n",
        "dogs_website_memos = dogs_website_memos.replace('<EOL>','\\n')\n",
        "print(dogs_website_memos[:5000])\n",
        "\n",
        "dogs_website_memos = [row for row in dogs_website_memos.split(sep='\\n')]\n",
        "dogs_website_memos = dogs_website_memos[1:] # Remove header\n",
        "dogs_website_memos = dogs_website_memos[:-1] # Remove last empty row\n",
        "\n",
        "dogs_memos = []\n",
        "for row in dogs_website_memos:\n",
        "    dogs_memos.append({\n",
        "        \"AnimalInternal-ID\": int(row.split(',\"')[0]),\n",
        "        \"MemoText\": row.split(',\"')[1]\n",
        "    })\n",
        "dogs_website_memos = pd.DataFrame(dogs_memos)\n",
        "print(\"Shape:\", dogs_website_memos.shape)\n",
        "dogs_website_memos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"AnimalInternal-ID\",\"MemoText\"\n",
            "1468738,\"Meet Cornell, he's a social butterfly deluxe and he loves him some human contact. Cornell would love to go home with someone who wants to give and receive a ton of love and who will also have lots of fun training this pup on the commands he'll use to be the best of the best.Cornell does really well in his crate and is house trained. He's great with other dogs and would be happy to have some doggie siblings in the house if you have some. He hasn't been observed with kitties to date and he is also good with kids.Cornell is a very food motivated, smart and very affectionate pup, and adores being with his people. He's also friendly to all people he encounters. He hasn't quite got the hang of fetch yet, but does like balls and toy time. He definitely thinks walk time is super fun, too. He's kind of a typical pup, loves wherever he is led in life and enjoys the recharge time snuggling up as well.Cornell would really be a happy young guy if you were to find him to be your choice for forever dog. Come meet him...we think that's exactly what you will want to do.\"\n",
            "1468727,\"Shaya is a puppy with potential extraordinaire. She is enjoying learning commands and already knows how to sit. She will be taking house training lessons, of course, but isn't there yet. She will settle down in her crate, but it's not her favorite place yet, so currently there might be a little protesting when sent to her room.Sometimes Shaya can be mistaken for a cat because she is quite the toy pouncer. It's so cute when they do that, don't you think?This young one gets along great with other dogs and with cats. She hasn't gotten to know any children yet. Though she is a sweet cuddler, her personality to date seems to be independent. She loves exploring on her own cause it's a big world out there to get to check out. She does enjoy playing with toys, but her attention span is puppy. She is spunky, playful and sweet as well as adept at learning. Sounds like quite little sweetheart just waiting for her forever home. Come meet her soon!\"\n",
            "1468736,\"Would you like to love good Luna? She's a wonderful adolescent who is ready to make the transition to adulthood in her new family's home.Luna has plenty of experience with both people and other dogs. She's very affectionate with all people. There is nothing she likes better than a good petting session. Luna has also spent a lot of time with other dogs and is a great playmate. She seems to get along with most every other dog.Luna generally stays fairly calm around the house, though she does have active spells. She's great on a leash and loves walking. She'd even be up to being your running partner! She enjoys a good romp with dog friends, and she can also entertain herself with her favorite toys. And once she's burned off some energy, she loves to get in some quality napping - preferably in her crate (she is crate trained).Luna would fit into most any type of family, though she would most enjoy one that could give her plenty of affection and activity. She loves children and would do well in a home with kids, though she is still learning to control her enthusiasm around children. She would also love to share a home with other dogs who could teach her the ways of the home and become playmates for her. This happy, fun, intelligent youngster should become a great long-term companion for the right family!Don't forget that APA's behavioral team will be available to help with any questions after Luna joins your family. Plus, remember that when you adopt her you'll also let us free up space so that another dog can join us at APA!\"\n",
            "1470308,\"Stanley is a seasoned elder statesman who still has some tread on the tires! He was found as a stray - though a dog of his age certainly hasn't been living on his own for his whole life. We're not sure exactly how old he is, but he's certainly been around the block a few times. Nonetheless, Stanley is in good health and is ready to live out his golden years in a comfy new home!Stanley is currently sharing a foster home with both dogs and cats, and has been a good housemate for all of them. They enjoy walks together, sniffing  the same things and each other. He's got good greeting skills with unfamiliar dogs, calmly leting them sniff and he isn't pushy about sniffing them. He's only startled if a cat hisses at him. What Stanley does enjoy is going for daily walks with his people. He's obviously not a young pup, but he keeps a good clip and has a spring in his step. At home, he enjoys being near people, but he doesn't demand attention. He's got a calm temperament and stays fairly quiet around the house.Of course, Stanley deserves a few creature comforts, and he really loves cozying up on a nice sofa or bed. If he has his own bed that would be great, but he'd also enjoy sharing yours!If you're looking to give a good home to a gentleman of a certain age, Stanley will definitely make it worth your while. Don't forget that APA's behavioral team wi\n",
            "Shape: (6487, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AnimalInternal-ID</th>\n",
              "      <th>MemoText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1468738</td>\n",
              "      <td>Meet Cornell, he's a social butterfly deluxe a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1468727</td>\n",
              "      <td>Shaya is a puppy with potential extraordinaire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1468736</td>\n",
              "      <td>Would you like to love good Luna? She's a wond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1470308</td>\n",
              "      <td>Stanley is a seasoned elder statesman who stil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1479031</td>\n",
              "      <td>Khaleesi is a purebred American Bulldog  that ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   AnimalInternal-ID                                           MemoText\n",
              "0            1468738  Meet Cornell, he's a social butterfly deluxe a...\n",
              "1            1468727  Shaya is a puppy with potential extraordinaire...\n",
              "2            1468736  Would you like to love good Luna? She's a wond...\n",
              "3            1470308  Stanley is a seasoned elder statesman who stil...\n",
              "4            1479031  Khaleesi is a purebred American Bulldog  that ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDcz5dPXZs4o",
        "outputId": "63961ecf-183b-4484-caea-bf52bf2b3b86"
      },
      "source": [
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "print(wrapper.fill(dogs_website_memos.iloc[2].MemoText))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Would you like to love good Luna? She's a wonderful adolescent who is ready to\n",
            "make the transition to adulthood in her new family's home.Luna has plenty of\n",
            "experience with both people and other dogs. She's very affectionate with all\n",
            "people. There is nothing she likes better than a good petting session. Luna has\n",
            "also spent a lot of time with other dogs and is a great playmate. She seems to\n",
            "get along with most every other dog.Luna generally stays fairly calm around the\n",
            "house, though she does have active spells. She's great on a leash and loves\n",
            "walking. She'd even be up to being your running partner! She enjoys a good romp\n",
            "with dog friends, and she can also entertain herself with her favorite toys. And\n",
            "once she's burned off some energy, she loves to get in some quality napping -\n",
            "preferably in her crate (she is crate trained).Luna would fit into most any type\n",
            "of family, though she would most enjoy one that could give her plenty of\n",
            "affection and activity. She loves children and would do well in a home with\n",
            "kids, though she is still learning to control her enthusiasm around children.\n",
            "She would also love to share a home with other dogs who could teach her the ways\n",
            "of the home and become playmates for her. This happy, fun, intelligent youngster\n",
            "should become a great long-term companion for the right family!Don't forget that\n",
            "APA's behavioral team will be available to help with any questions after Luna\n",
            "joins your family. Plus, remember that when you adopt her you'll also let us\n",
            "free up space so that another dog can join us at APA!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32YsimM3bSf4"
      },
      "source": [
        "#### dogs_qa.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "KEYB9cnEbS-Z",
        "outputId": "46455508-5feb-4f1c-9afa-d37f99b7c4b1"
      },
      "source": [
        "# dogs_qa.csv\n",
        "dogs_qa_path = os.path.join(datasets_path,\"dogs_qa.csv\")\n",
        "dogs_qa = pd.read_csv(dogs_qa_path)\n",
        "print(\"Shape:\",dogs_qa.shape)\n",
        "dogs_qa.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (499, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>breed</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Terrier, Pit Bull/Mix</td>\n",
              "      <td>Are Pitbull Terriers good family dogs?</td>\n",
              "      <td>When raised with the proper training and socia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Terrier, Pit Bull/Mix</td>\n",
              "      <td>Does terrier mix mean pit bull?</td>\n",
              "      <td>A terrier mix combines one parent from a terri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Terrier, Pit Bull/Mix</td>\n",
              "      <td>What dog will kill a pitbull?</td>\n",
              "      <td>So, what dog can beat a Pitbull? A Rottweiler ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Terrier, Pit Bull/Mix</td>\n",
              "      <td>Do pitbulls like to cuddle?</td>\n",
              "      <td>Even if a Pit Bull does not like other dogs, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Terrier, Pit Bull/Mix</td>\n",
              "      <td>Do pitbulls turn on their owners?</td>\n",
              "      <td>They can become aggressive and if you have an ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   breed  ...                                             answer\n",
              "0  Terrier, Pit Bull/Mix  ...  When raised with the proper training and socia...\n",
              "1  Terrier, Pit Bull/Mix  ...  A terrier mix combines one parent from a terri...\n",
              "2  Terrier, Pit Bull/Mix  ...  So, what dog can beat a Pitbull? A Rottweiler ...\n",
              "3  Terrier, Pit Bull/Mix  ...  Even if a Pit Bull does not like other dogs, t...\n",
              "4  Terrier, Pit Bull/Mix  ...  They can become aggressive and if you have an ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jgwUz6R1iii"
      },
      "source": [
        "##### bad_words.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUTBzzmA1luU",
        "outputId": "72aaa1b2-53de-4fcf-c693-a6680ceb6a93"
      },
      "source": [
        "# bad_words.csv\n",
        "bad_words_path = os.path.join(datasets_path,\"bad_words.csv\")\n",
        "bad_words = pd.read_csv(bad_words_path,header=None)\n",
        "print(\"Shape:\",bad_words.shape)\n",
        "bad_words_list = bad_words[0].values.tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (451, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VdiQPNCuDKI"
      },
      "source": [
        "## **GPT2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myw1A9JcmD2s"
      },
      "source": [
        "#### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzbJI8KtmHd3"
      },
      "source": [
        "Comparing GPT2 with BERT:\n",
        "\n",
        "<table>\n",
        "<tr><td width=\"400\"><strong>GPT2</strong></td><td width=\"400\"><strong>BERT</strong><td></tr>\n",
        "<tr><td>Auto-regressive model (A word is predicted using words from its left context only)</td><td>Masked Language Model</td></tr>\n",
        "<tr><td>Made up of only the Decoder with stacked transformer blocks</td><td>Made up of only the Encoder with stacked transformer blocks</td></tr>\n",
        "\n",
        "<tr><td>Unidirectional language model</td><td>Bidirectional language model</td></tr>\n",
        "<tr><td>Good for writing text</td><td>Good for fill in the blanks</td></tr>\n",
        "</table>\n",
        "\n",
        "<br>  \n",
        "\n",
        "\n",
        "**Language Model**: \n",
        "\n",
        "A model that understands language and how words appear in context to one another. The model is trained using unsupervised approaches such as next word prediction in a sentence or next sentence prediction.\n",
        "\n",
        "**Question Answering Model**: \n",
        "\n",
        "In the most common terminology, they are models that can find an answer when given a context text. Similar to reading comprehension \n",
        "\n",
        "**Dialog Model**: \n",
        "\n",
        "A model that you can converse with. It keeps track of the context/history and can identify user intents and provide specific answers. E.g: Chatbot\n",
        "\n",
        "<br> \n",
        "\n",
        "We want to build a model that is capable of having a dialog/conversation in a natural way. For this we will attempt to use the GPT2 model. GPT2 was trained on 40GB of Internet text and understand language very well. \n",
        "First we will try to use the pretrained GPT2 out of the box and then we will fine tune with just one dogs data to see how a pre trained language model can be adpated to a custom dataset.\n",
        "\n",
        "We will perform this task using the pretrained GPT2 model from the library <strong>transformers</strong>:\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/gpt2finetuning01.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hed9HtI0uJKE"
      },
      "source": [
        "#### Load Pretrained Model/Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyb_C5xUuLwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "4dceba7b19ea446399dd5e3173f4c76e",
            "ee89208102134e76a586263c73e81872",
            "61b39a0b4b2046d5a40c108a720fc39f",
            "546e4e91c4934344b9cd8d89edb3536c",
            "ad9fdf36e2284701a42eabea88fc64e4",
            "56064ac5495749509225eb957db4112a",
            "4351a713794a44de9e984270560f4c43",
            "7f3214bbef6c43059c8f8ff797524715",
            "9a55c90f17914093be405fb2b784ce40",
            "6e6f529563f343ca8f8dc21c523823f5",
            "7d1ede52900941fe983db6e403a1cfd8",
            "3895dabfb7d04403a087e70e571e70d3",
            "7f994f8d3f3d4c7a93f2efb897eb8953",
            "8bfcad86da7147559980391433a3207c",
            "93471fb2a24444f0ac856a3dd7988c25",
            "7f2a2a00fc5543c9be86c69ad7183bc3",
            "f6767f25b7024527a41726b6cc2d6310",
            "efec208ca3294b409f267bb369c2baf6",
            "e9597de56e34446e91806dcabfe8b925",
            "7add849b68b34208b79c09cecf0f2b0d",
            "9cffc9cec138409abef31860dc3eca2f",
            "39569b10070a4a3b8bf4e7e289fed375",
            "67f94530bd1c48f2bd46e1688ea9217d",
            "343d12929c0c41dab38cc5a8278e1241",
            "def2618a93234c7a92a5d83cf195c6ec",
            "89e29c5111014b3da0e2e55d6703271a",
            "1ea088a9b6314aff95d369fc05d64d67",
            "4cf01b2b906d483e9bb6c12a46fa9c6c",
            "b647a90df1254b88ba92c54d4f6ecb2c",
            "353321b5f36a4042ab371c13d39d9cb6",
            "b1ea13eb17ec40a58d64e8ec760b1846",
            "6fe3beec9cf349b1ad42b9b59a43e954"
          ]
        },
        "outputId": "1e09bd66-ba80-4214-8284-cd652a09ca79"
      },
      "source": [
        "# load pretrained gpt2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482321795336 acquired on /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dceba7b19ea446399dd5e3173f4c76e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482321795336 released on /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482471119672 acquired on /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a55c90f17914093be405fb2b784ce40",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482471119672 released on /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482321796400 acquired on /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6767f25b7024527a41726b6cc2d6310",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482321796400 released on /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482300978120 acquired on /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "def2618a93234c7a92a5d83cf195c6ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140482300978120 released on /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiW12cHa2bSq"
      },
      "source": [
        "# Generate token for bad words\n",
        "bad_words_tokens = [tokenizer.encode(x, add_special_tokens=False) for x in bad_words_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq0710F3uj_t"
      },
      "source": [
        "#### Language generation without finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0tuS2JlumBf",
        "outputId": "9a437206-d1dc-4095-8cbc-a6454de8c4ca"
      },
      "source": [
        "# Tokenize input text\n",
        "input_ids = tokenizer.encode(\"Is Emma a good dog?\", return_tensors='pt')\n",
        "print(\"input_ids\",input_ids)\n",
        "# Use model to generate text\n",
        "output = model.generate(input_ids, \n",
        "                        max_length=40, \n",
        "                        num_return_sequences=5, \n",
        "                        do_sample=True, \n",
        "                        early_stopping=True,\n",
        "                        bad_words_list=bad_words_tokens)\n",
        "print(\"Generated text:\")\n",
        "print('---------------------------------------------')\n",
        "for i in range(len(output)):\n",
        "  print(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "  print('---------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_ids tensor([[ 3792, 18966,   257,   922,  3290,    30]])\n",
            "Generated text:\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? The answer, unfortunately, might not be a sure thing. As a pet for over 100 years, Emma was an incredibly important part of Emma Thompson's life. For an\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? Oh, how she'd like to be. \"I've been working on it for five months, and this thing is awesome.\"\n",
            "\n",
            "Diana's favorite treats are\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? Does she make her friends better? Or is she a coward or a hater? The latter questions every dog of any breed, from Labrador to Maltese to American Shepherd\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "Let's take a look at the data we have on Emma's breedable size, because it's not very interesting. As an example, we looked at the\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? This is not a problem. This is the problem with all the dogs that have been taught to love dogs.\n",
            "\n",
            "So if you want Emma to love you, what\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTu6p5LKvbSR"
      },
      "source": [
        "#### Finetuning GPT2\n",
        "\n",
        "We see that the GPT2 model is not able to generate any meaningful text to the context of our problem. So we will use Transfer learning help us solve this problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td9JPHOXwhFv"
      },
      "source": [
        "#### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3QGfSWyb7cu"
      },
      "source": [
        "##### Meet Emma\n",
        "\n",
        "Pull just Emma's data to explore the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "FPMSo0u-b94V",
        "outputId": "b29f7763-ccc4-4dde-8cb5-51ae30104d68"
      },
      "source": [
        "# 1444011\n",
        "animal_id = 1444011\n",
        "emma_data = dogs[dogs[\"AnimalInternal-ID\"] == animal_id]\n",
        "print('Metadata:')\n",
        "display(emma_data)\n",
        "print('Memo about dog:')\n",
        "print(wrapper.fill(dogs_website_memos[dogs_website_memos[\"AnimalInternal-ID\"] == animal_id][\"MemoText\"].values[0]))\n",
        "\n",
        "print('Common question/answers from Google about the breed:')\n",
        "breed_qa = dogs_qa[dogs_qa[\"breed\"] == \"Retriever, Labrador/Mix\"]\n",
        "print(breed_qa.question.iloc[0])\n",
        "print(wrapper.fill(breed_qa.answer.iloc[0]))\n",
        "print('\\n')\n",
        "print(breed_qa.question.iloc[1])\n",
        "print(wrapper.fill(breed_qa.answer.iloc[1]))\n",
        "print('\\n')\n",
        "print(breed_qa.question.iloc[2])\n",
        "print(wrapper.fill(breed_qa.answer.iloc[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Metadata:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AnimalID</th>\n",
              "      <th>AnimalInternal-ID</th>\n",
              "      <th>AnimalName</th>\n",
              "      <th>AnimalType</th>\n",
              "      <th>AnimalSex</th>\n",
              "      <th>AnimalCurrentWeightPounds</th>\n",
              "      <th>AnimalDOB</th>\n",
              "      <th>AnimalBreed</th>\n",
              "      <th>AnimalColor</th>\n",
              "      <th>AnimalPattern</th>\n",
              "      <th>DOB</th>\n",
              "      <th>Year</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45628</td>\n",
              "      <td>1444011</td>\n",
              "      <td>Emma</td>\n",
              "      <td>Dog</td>\n",
              "      <td>Female</td>\n",
              "      <td>53.3</td>\n",
              "      <td>20150306</td>\n",
              "      <td>Retriever, Yellow Labrador /Mix</td>\n",
              "      <td>Blond /None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-03-06</td>\n",
              "      <td>2015</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   AnimalID  AnimalInternal-ID AnimalName  ...        DOB  Year  Age\n",
              "0     45628            1444011       Emma  ... 2015-03-06  2015  5.0\n",
              "\n",
              "[1 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Memo about dog:\n",
            "Emma is a blonde princess who definitely likes the finer things in life - like\n",
            "kisses and hugs from her humans. She is so happy to be alive and even happier\n",
            "when her hair can be flying out a car window!This very affectionate, funny girl,\n",
            "now in foster care, always wants to start her day by getting those hugs and\n",
            "kisses from her person. And then she's ready for a day of playing with her\n",
            "brother and her foster dog buddy and playing fetch and tug-o-war.When playtime\n",
            "is done, Emma is also ready for her schooling. This spontaneous girl is a fast\n",
            "learner who already answers to the Come, Stop, No commands and is working on\n",
            "Sit, Stay and Place. She's crate trained, house trained and is always happy to\n",
            "match her person's activity level - up for a morning jog, then hanging out on\n",
            "the couch. And she's sure she's the best smeller in the world - bloodhounds and\n",
            "police dogs have nothing on her!Emma is a spirited, eager-to-please girl who's\n",
            "perfect for a family who wants a companion on those walks and a co-pilot for\n",
            "those drives in the car. Come take her for a walk and see if she's perfect for\n",
            "your home.\"\n",
            "Common question/answers from Google about the breed:\n",
            "How big does a Labrador retriever mix get?\n",
            "Males stand 22.5 to 24.5 inches, and weigh 65 to 80 pounds. Females stand 21.5\n",
            "to 23.5 inches, and weigh 55 to 70 pounds.\n",
            "\n",
            "\n",
            "Are lab mix good dogs?\n",
            "In fact, in most cases, mixed breed dogs, like Lab terriers or Lab Border Collie\n",
            "mixes, can be awesome family pets, and they might often be better off health-\n",
            "wise than their purebred counterparts.\n",
            "\n",
            "\n",
            "Does a Labrador retriever mix shed?\n",
            "This mix breed dog can weigh 35 to 80 pounds and doesn't need much grooming to\n",
            "stay clean. However, you can expect year-round shedding and heavier sheds\n",
            "seasonally.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruWvJHkqfVog"
      },
      "source": [
        "##### Generating QA Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcUbokmLfYPk"
      },
      "source": [
        "def generate_qa_text(dog_data,breed_qa):\n",
        "  questions = [\n",
        "  ['What is her name?',\n",
        "    'What is {}\\'s type?'.format(name),\n",
        "    'What is {}\\'s gender?'.format(name),\n",
        "    'What is {}\\'s weight?'.format(name),\n",
        "    'What is {}\\'s date of birth?'.format(name),\n",
        "   'When was {} born?'.format(name),\n",
        "   'What is {}\\'s age?'.format(name),\n",
        "    'What is {}\\'s breed?'.format(name),\n",
        "    'What is {}\\'s color?'.format(name),\n",
        "    ]\n",
        "\n",
        "    for name, a_type, a_sex, weight, dob, year, age, breed, color in \n",
        "        zip(dog_data.AnimalName, \n",
        "            dog_data.AnimalType, \n",
        "            dog_data.AnimalSex, \n",
        "            dog_data.AnimalCurrentWeightPounds,\n",
        "            dog_data.DOB,\n",
        "            dog_data.Year,\n",
        "            dog_data.Age,\n",
        "            dog_data.AnimalBreed,\n",
        "            dog_data.AnimalColor)\n",
        "  ]\n",
        "\n",
        "  answers = [\n",
        "  ['Her name is {}'.format(name),\n",
        "    '{} is a {}'.format(name, a_type),\n",
        "    '{} is {}'.format(name, a_sex),\n",
        "    '{}\\'s weight is {}'.format(name, weight),\n",
        "    '{}\\'s date of birth is {}'.format(name, dob),\n",
        "   '{}\\ was born on {}'.format(name, year),\n",
        "   '{}\\ is {} years old'.format(name, age),\n",
        "    '{}\\'s breed is {}'.format(name, breed),\n",
        "    '{}\\'s color is {}'.format(name, color),\n",
        "    ]\n",
        "\n",
        "    if a_sex=='Female'\n",
        "\n",
        "    else\n",
        "\n",
        "    ['His name is {}'.format(name),\n",
        "    '{} is a {}'.format(name, a_type),\n",
        "    '{} is {}'.format(name, a_sex),\n",
        "    '{}\\'s weight is {}'.format(name, weight),\n",
        "    '{}\\'s date of birth is {}'.format(name, dob),\n",
        "   '{}\\ was born on {}'.format(name, year),\n",
        "   '{}\\ is {} years old'.format(name, age),\n",
        "    '{}\\'s breed is {}'.format(name, breed),\n",
        "    '{}\\'s color is {}'.format(name, color),\n",
        "    ]\n",
        "\n",
        "    for name, a_type, a_sex, weight, dob,year, age, breed, color in \n",
        "        zip(dog_data.AnimalName, \n",
        "            dog_data.AnimalType, \n",
        "            dog_data.AnimalSex, \n",
        "            dog_data.AnimalCurrentWeightPounds,\n",
        "            dog_data.DOB,\n",
        "            dog_data.Year,\n",
        "            dog_data.Age,\n",
        "            dog_data.AnimalBreed,\n",
        "            dog_data.AnimalColor)\n",
        "  ]\n",
        "\n",
        "  qa_text_df = pd.DataFrame({'question': questions[0], 'answer': answers[0]})\n",
        "\n",
        "  qa_text_df = qa_text_df.append(breed_qa[['question', 'answer']])\n",
        "  qa_text_df = qa_text_df.reset_index(drop=True)\n",
        "  return qa_text_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "wrJwC1AzglU-",
        "outputId": "012971f2-b2b0-4bdc-c508-f4485ca82209"
      },
      "source": [
        "emma_df = generate_qa_text(emma_data,breed_qa)\n",
        "print(\"Shape:\",emma_df.shape)\n",
        "emma_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (38, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is her name?</td>\n",
              "      <td>Her name is Emma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is Emma's type?</td>\n",
              "      <td>Emma is a Dog</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is Emma's gender?</td>\n",
              "      <td>Emma is Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is Emma's weight?</td>\n",
              "      <td>Emma's weight is 53.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is Emma's date of birth?</td>\n",
              "      <td>Emma's date of birth is 2015-03-06 00:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        question                                       answer\n",
              "0              What is her name?                             Her name is Emma\n",
              "1           What is Emma's type?                                Emma is a Dog\n",
              "2         What is Emma's gender?                               Emma is Female\n",
              "3         What is Emma's weight?                        Emma's weight is 53.3\n",
              "4  What is Emma's date of birth?  Emma's date of birth is 2015-03-06 00:00:00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_TgT4yMikoWn",
        "outputId": "ed9d4596-856d-4b84-f1e1-3bd499e9bf5c"
      },
      "source": [
        "emma_df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>What is the smallest breed of Labrador?</td>\n",
              "      <td>Besides being smaller in size, miniature labra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>What is a Labrador and golden retriever mix ca...</td>\n",
              "      <td>Loving, devoted, and energetic, Goldador mixed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>How can I tell if my lab is mixed?</td>\n",
              "      <td>Lab Mixed Breeds The best way to tell the diff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>What is a lab hound mix called?</td>\n",
              "      <td>The Bassador is a mixed breed dog–a cross betw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>What breed of dog goes well with a Labrador?</td>\n",
              "      <td>Boston Terrier. This is one of the breeds that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question                                             answer\n",
              "33            What is the smallest breed of Labrador?  Besides being smaller in size, miniature labra...\n",
              "34  What is a Labrador and golden retriever mix ca...  Loving, devoted, and energetic, Goldador mixed...\n",
              "35                 How can I tell if my lab is mixed?  Lab Mixed Breeds The best way to tell the diff...\n",
              "36                    What is a lab hound mix called?  The Bassador is a mixed breed dog–a cross betw...\n",
              "37       What breed of dog goes well with a Labrador?  Boston Terrier. This is one of the breeds that..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCK0Vo-IwjER"
      },
      "source": [
        "# Save to text file\n",
        "emma_df.to_csv('emma.txt', header=None, index=None, sep=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlc49nXVwdDf"
      },
      "source": [
        "#### Train\n",
        "\n",
        "We will use a script from the Huggingface library to finetune our model. There are a lot of well written scripts to train language models in the following repo:\n",
        "\n",
        "https://github.com/huggingface/transformers/tree/master/examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acS24yBowTjf",
        "outputId": "a53374fd-306c-429b-c6fe-2acc023895d7"
      },
      "source": [
        "# Download run_clm.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-19 15:28:58--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17046 (17K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "run_clm.py          100%[===================>]  16.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-19 15:28:58 (119 MB/s) - ‘run_clm.py’ saved [17046/17046]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G91LoLDwwgPd",
        "outputId": "2532f5d8-44f1-4257-9c93-d27b206be00f"
      },
      "source": [
        "!python run_clm.py \\\n",
        "    --output_dir='emma_model/' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_file='emma.txt' \\\n",
        "    --do_eval \\\n",
        "    --validation_file='emma.txt' \\\n",
        "    --per_device_train_batch_size 1  \\\n",
        "    --num_train_epochs 50 \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 15:29:04.534844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/19/2021 15:29:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/19/2021 15:29:05 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=emma_model/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan19_15-29-05_3c45f7627b2a, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=emma_model/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 3.01MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-2434b925db334214 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-2434b925db334214/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-2434b925db334214/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:445] 2021-01-19 15:29:07,065 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:481] 2021-01-19 15:29:07,066 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:445] 2021-01-19 15:29:07,333 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:481] 2021-01-19 15:29:07,333 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/19/2021 15:29:08 - INFO - filelock -   Lock 140201787653144 acquired on /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|file_utils.py:1272] 2021-01-19 15:29:08,158 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpltj4z011\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 2.63MB/s]\n",
            "[INFO|file_utils.py:1276] 2021-01-19 15:29:08,951 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1279] 2021-01-19 15:29:08,951 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "01/19/2021 15:29:08 - INFO - filelock -   Lock 140201787653144 released on /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|tokenization_utils_base.py:1766] 2021-01-19 15:29:08,952 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1766] 2021-01-19 15:29:08,952 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1766] 2021-01-19 15:29:08,952 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-01-19 15:29:09,280 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-01-19 15:29:13,933 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-01-19 15:29:13,934 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 140.62ba/s]\n",
            "100% 1/1 [00:00<00:00, 227.26ba/s]\n",
            "100% 1/1 [00:00<00:00, 347.61ba/s]\n",
            "100% 1/1 [00:00<00:00, 357.14ba/s]\n",
            "[INFO|trainer.py:442] 2021-01-19 15:29:22,029 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:442] 2021-01-19 15:29:22,029 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:791] 2021-01-19 15:29:22,032 >> ***** Running training *****\n",
            "[INFO|trainer.py:792] 2021-01-19 15:29:22,033 >>   Num examples = 1\n",
            "[INFO|trainer.py:793] 2021-01-19 15:29:22,033 >>   Num Epochs = 50\n",
            "[INFO|trainer.py:794] 2021-01-19 15:29:22,033 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:795] 2021-01-19 15:29:22,033 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:796] 2021-01-19 15:29:22,033 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:797] 2021-01-19 15:29:22,033 >>   Total optimization steps = 50\n",
            "100% 50/50 [00:11<00:00,  4.48it/s][INFO|trainer.py:953] 2021-01-19 15:29:33,339 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 11.3068, 'train_samples_per_second': 4.422, 'epoch': 50.0}\n",
            "100% 50/50 [00:11<00:00,  4.43it/s]\n",
            "[INFO|trainer.py:1344] 2021-01-19 15:29:33,342 >> Saving model checkpoint to emma_model/\n",
            "[INFO|configuration_utils.py:300] 2021-01-19 15:29:33,344 >> Configuration saved in emma_model/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-19 15:29:35,132 >> Model weights saved in emma_model/pytorch_model.bin\n",
            "01/19/2021 15:29:35 - INFO - __main__ -   ***** Train results *****\n",
            "01/19/2021 15:29:35 - INFO - __main__ -     epoch = 50.0\n",
            "01/19/2021 15:29:35 - INFO - __main__ -     train_runtime = 11.3068\n",
            "01/19/2021 15:29:35 - INFO - __main__ -     train_samples_per_second = 4.422\n",
            "01/19/2021 15:29:35 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1536] 2021-01-19 15:29:35,218 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1537] 2021-01-19 15:29:35,218 >>   Num examples = 1\n",
            "[INFO|trainer.py:1538] 2021-01-19 15:29:35,219 >>   Batch size = 8\n",
            "100% 1/1 [00:00<00:00, 546.92it/s]\n",
            "01/19/2021 15:29:35 - INFO - __main__ -   ***** Eval results *****\n",
            "01/19/2021 15:29:35 - INFO - __main__ -     perplexity = 1.26365109816037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKd426dMw8kL"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qgP13qxxCJa"
      },
      "source": [
        "# Load the finetuned model and tokenizer from the training step\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./emma_model/')\n",
        "model = GPT2LMHeadModel.from_pretrained('./emma_model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWOj3kTP4Sju"
      },
      "source": [
        "# Generate token for bad words\n",
        "bad_words_tokens = [tokenizer.encode(x, add_special_tokens=False) for x in bad_words_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDyh2eh2z0P_",
        "outputId": "7f72cb98-1c8d-4ef1-e65c-eb7b08cf2114"
      },
      "source": [
        "# Tokenize input text\n",
        "input_ids = tokenizer.encode(\"Is Emma a good dog?\", return_tensors='pt')\n",
        "print(\"input_ids\",input_ids)\n",
        "# Use model to generate text\n",
        "output = model.generate(input_ids, \n",
        "                        max_length=30, \n",
        "                        num_return_sequences=5, \n",
        "                        do_sample=True, \n",
        "                        temperature=1,\n",
        "                        early_stopping=True,\n",
        "                        bad_words_list=bad_words_tokens)\n",
        "print(\"Generated text:\")\n",
        "print('---------------------------------------------')\n",
        "for i in range(len(output)):\n",
        "  print(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "  print('---------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_ids tensor([[ 3792, 18966,   257,   922,  3290,    30]])\n",
            "Generated text:\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "For anyone who has spent any time in animal shelter, you are about to find yourself in a situation where you\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? Or an easy fix for her problems?\n",
            "\n",
            "The answers are many — and very surprising, like she could do any\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "Is this a new thing? How would a dog like her own life fit in? (Note: I believe\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? Well, why not.\n",
            "\n",
            "If Emma could be a better person, the girl would need help becoming a more mature\n",
            "---------------------------------------------\n",
            "Is Emma a good dog? When the book becomes available on Kindle.\n",
            "\n",
            "I wrote the book myself and it took me almost three years to finish\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOQ0OLY8lhI_"
      },
      "source": [
        "Results are ok, not so goo. Now let us reduce the temperature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drUt3X6i0K_D",
        "outputId": "c9144254-41d1-48fd-f76e-39a4acd78a4e"
      },
      "source": [
        "# Reduce the temperature when generating text\n",
        "output = model.generate(input_ids, \n",
        "                        max_length=30, \n",
        "                        num_return_sequences=5, \n",
        "                        do_sample=True, \n",
        "                        temperature=0.3,\n",
        "                        early_stopping=True,\n",
        "                        bad_words_list=bad_words_tokens)\n",
        "print(\"Generated text:\")\n",
        "print('---------------------------------------------')\n",
        "\n",
        "for i in range(len(output)):\n",
        "  print(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "  print('---------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated text:\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "I'm not sure. I'm not sure if she's a good dog.\n",
            "\n",
            "I'm not sure\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "I think so.\n",
            "\n",
            "I'm not sure if Emma is a good dog, but I think she's\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "I'm not sure if she's a good dog, but I think she's a good dog.\n",
            "\n",
            "\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "I don't know, but I think Emma is a good dog. She's a good dog, and she\n",
            "---------------------------------------------\n",
            "Is Emma a good dog?\n",
            "\n",
            "No.\n",
            "\n",
            "I am not a dog.\n",
            "\n",
            "I am a human.\n",
            "\n",
            "I am not\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdOYj_ZalrMa"
      },
      "source": [
        "Let's try another question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRJjyvkrDg6g",
        "outputId": "e6714dd7-61d6-4923-bfa6-baa75a5ac303"
      },
      "source": [
        "# Tokenize inputs\n",
        "input_ids = tokenizer.encode(\"What is Emma's breed?\", return_tensors='pt')\n",
        "print(\"input_ids\",input_ids)\n",
        "# Use model to generate text\n",
        "output = model.generate(input_ids, \n",
        "                        max_length=30, \n",
        "                        num_return_sequences=5, \n",
        "                        do_sample=True, \n",
        "                        temperature=0.3,\n",
        "                        early_stopping=True,\n",
        "                        bad_words_list=bad_words_tokens)\n",
        "print(\"Generated text:\")\n",
        "print('---------------------------------------------')\n",
        "\n",
        "for i in range(len(output)):\n",
        "  print(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "  print('---------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_ids tensor([[ 2061,   318, 18966,   338, 15939,    30]])\n",
            "Generated text:\n",
            "---------------------------------------------\n",
            "What is Emma's breed?\n",
            "\n",
            "Emma's breed is a hybrid of the two breeds. Emma's breed is a hybrid of the two breeds\n",
            "---------------------------------------------\n",
            "What is Emma's breed?\n",
            "\n",
            "Emma is a black Labrador Retriever. She is a very intelligent and loving dog. She is very\n",
            "---------------------------------------------\n",
            "What is Emma's breed?\n",
            "\n",
            "Emma is a male with a very short tail. She has a short, dark brown hairline and a\n",
            "---------------------------------------------\n",
            "What is Emma's breed?\n",
            "\n",
            "Emma is a very rare breed. It is not a breed that is bred for the sake of breeding.\n",
            "---------------------------------------------\n",
            "What is Emma's breed?\n",
            "\n",
            "Emma is a very rare breed in the UK. It is only found in the UK, and is not\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muaD3qJcDzBi",
        "outputId": "6785052f-7430-4116-fd80-6f0c87a2dc66"
      },
      "source": [
        "# Tokenize inputs\n",
        "input_ids = tokenizer.encode(\"What does Emma enjoy doing?\", return_tensors='pt')\n",
        "print(\"input_ids\",input_ids)\n",
        "# Use model to generate text\n",
        "output = model.generate(input_ids, \n",
        "                        max_length=30, \n",
        "                        num_return_sequences=5, \n",
        "                        do_sample=True, \n",
        "                        top_k=25,\n",
        "                        top_p=0.45,\n",
        "                        early_stopping=True,\n",
        "                        bad_words_list=bad_words_tokens)\n",
        "print(\"Generated text:\")\n",
        "\n",
        "for i in range(len(output)):\n",
        "  print(tokenizer.decode(output[i], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_ids tensor([[ 2061,   857, 18966,  2883,  1804,    30]])\n",
            "Generated text:\n",
            "What does Emma enjoy doing?\n",
            "\n",
            "Emma is a very nice girl, and she loves to play with herself. She's a very good dancer\n",
            "What does Emma enjoy doing? She loves to do it. She loves to be a part of the team. She loves to be a part of the\n",
            "What does Emma enjoy doing?\n",
            "\n",
            "Emma is a very active and creative person. She enjoys reading and writing, and she enjoys writing and writing\n",
            "What does Emma enjoy doing? She loves to read and write. She enjoys playing with her toys and doing whatever she can to make her life better.\n",
            "What does Emma enjoy doing?\n",
            "\n",
            "Emma enjoys being a part of the community. She loves to talk about things and be a part of it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv8Wrqr8EfNA"
      },
      "source": [
        "#### Nano Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzc85A_cYjPB"
      },
      "source": [
        "##### Question 1\n",
        "\n",
        "* **Why did GPT2 fail at answering specific questions??**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5FxSm6yYwfk"
      },
      "source": [
        "##### Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYy1pMWyYxCU"
      },
      "source": [
        "GPT2 is a generative model and hence generates a sequence of words that are most likely to occur after the given prompt (question in this case). Although most of the time the anwer does exist in the generated text after the question, the model is unable to give a specific answer to the question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr9VowfCWQz_"
      },
      "source": [
        "##### Question 2\n",
        "\n",
        "* **How does temperature help in generating answers?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0yqYqNRXG-A"
      },
      "source": [
        "##### Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlJO624iXJUF"
      },
      "source": [
        "A lower temperature drags large probabilities closer to 1, small probabilities closer to 0. So words with higher importance would be used ot generate the next word in the sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Qdc4DtWhjS"
      },
      "source": [
        "##### Question 3\n",
        "\n",
        "* **How does the answers generated by GPT2 differ from the answers generated by BERT from Lab 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJcLEbUX4Wr"
      },
      "source": [
        "##### Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHt_U13AX4ya"
      },
      "source": [
        "BERT found an **exact match** of the answer from a given context. GPT2 **generated** an answer based on the given prompt. GPT2's generated answer need not be an exact sequence of words that exist in the original data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmg4Oq31Gl1S"
      },
      "source": [
        "## **GPT2 Double Head Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNMQ1VubG3sG"
      },
      "source": [
        "#### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfxdC7mlG1eh"
      },
      "source": [
        "We have seen how a Question Answering model works, we also saw how a Language generation model works. Let's attempt to combine some these ideas from the two models into one that can both answer questions as well as generate them. For this we will extend the GPT2 model.\n",
        "\n",
        "**Causal Transformer**: \n",
        "\n",
        "We saw that GPT2 is the made up of only the Decoder with stacked transformer blocks. Also the model predicts words using only words from the left context. So if we look at our example on Emma.\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/causaltransformer02.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX3GNzuqmMHG"
      },
      "source": [
        "**Double Head Model**: \n",
        "\n",
        "Now how do we adapt this language model into a dialog task? In a question answering model we had to feed in a context and the model returned an answer. The language model generated text based on previous words. So if use the GPT2 model as a base and for the input we add some context to the data such as:\n",
        "- Information about the dog, or its `persona`\n",
        "- The `history` of the dialogue with the user\n",
        "- The `answer` of the dog\n",
        "\n",
        "And as a head we add:\n",
        "- Language Model Head\n",
        "- Multiple Choice Head\n",
        "\n",
        "The GPT2 has by default one language model head which takes the hidden states from the final transform block and pass it to a linear layer to compute the logits. We then add another head called mutiple choice head, which takes the hidden states from the final transform block and summarizes the sequences to a single vector of a sequence hidden states. This could be done using `last` which is to take the last token hidden state, or `first` which is to take the first token hidden state, or `mean` which is to take the mean of all tokens hidden states.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/gpt2doubleheadmodel.png\" />\n",
        "\n",
        "\n",
        "**Word Embeddings**: Word embeddings are where each word in the dataset is mapped to a numberical vector. Each of these vector has a sense of context between the words. So for exmaple words with simialr meaning or concepts come together in the vector space.\n",
        "\n",
        "**Positional Embedding**: A transformer based model has no sense of the sequence of an input. So to give the model some sense of order we add a piece of information to each word about its position in the sentence. So positional embedding is a n-dimensional vector that contains information about a specific position in a sentence.\n",
        "\n",
        "**Segment Embedding**: Our input consists of persona, history, and answer. So we want add information about each segment in the input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6aszzg3GtLs"
      },
      "source": [
        "**Finetuning Options**: \n",
        "\n",
        "There are multiple options to perform transfer learning and finetuing for our final dialog model:\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/gpt2dhfinetuning01.png\" width=\"800\"/>\n",
        "\n",
        "- PERSONA-CHAT dataset size - 17,000\n",
        "- Our dog dataset (small) 800"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6jgc9PG82c"
      },
      "source": [
        "#### Load Pretrained Model/Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st43ZprOHDZR",
        "outputId": "7c07f80f-d3c0-4056-c14d-0404cc18cf19"
      },
      "source": [
        "model_url = \"https://computefest2021images.s3.amazonaws.com/language_models/trained_model_epochs_1.zip\"\n",
        "start_time = time.time()\n",
        "download_file(model_url, base_path=\"models\", extract=True)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "logger.info(\"Download execution time (mins): %s\",execution_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:.:Download execution time (mins): 0.29630059003829956\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6BSkD0nHMZb"
      },
      "source": [
        "# Load trained model\n",
        "model = GPT2DoubleHeadsModel.from_pretrained(\"./models/trained_model/\")\n",
        "# Convert model parameter tensors to CUDA tensors\n",
        "model.to(device)\n",
        "# Load trained Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./models/trained_model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rsy_MyyH0nN"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6-EQvda9ql1"
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {\n",
        "    \"bos_token\": \"<bos>\",\n",
        "    \"eos_token\": \"<eos>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
        "}\n",
        "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4141fs_H3Xm"
      },
      "source": [
        "# Utils for tokenization & data preparation\n",
        "process_count = 1\n",
        "multiprocessing_chunksize = 500\n",
        "\n",
        "def tokenize_multi(data):\n",
        "  obj, tokenizer = data\n",
        "  if isinstance(obj, str):\n",
        "      return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj, dict):\n",
        "      return dict((n, tokenize_multi((o, tokenizer))) for n, o in obj.items())\n",
        "  return list(tokenize_multi((o, tokenizer)) for o in obj)\n",
        "\n",
        "def tokenize(obj):\n",
        "  if isinstance(obj, str):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj, dict):\n",
        "    return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "\n",
        "  data = [(d, tokenizer) for d in obj]\n",
        "  with Pool(process_count) as p:\n",
        "    tokenized_data = list(\n",
        "        tqdm(p.imap(tokenize_multi, data, chunksize=multiprocessing_chunksize), total=len(data))\n",
        "    )\n",
        "  return tokenized_data\n",
        "\n",
        "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
        "  \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "  bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "  sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
        "  sequence = [sequence[0]] + [\n",
        "      [speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])\n",
        "  ]\n",
        "  instance = {}\n",
        "  instance[\"input_ids\"] = list(chain(*sequence))\n",
        "  instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "  instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "  instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
        "  if lm_labels:\n",
        "      instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "  return instance\n",
        "\n",
        "def pad_dataset(dataset, padding=0):\n",
        "  \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level,\n",
        "  but this is simpler. \"\"\"\n",
        "  max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "  for name in PADDED_INPUTS:\n",
        "      dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "  return dataset\n",
        "\n",
        "def prepare_datasets(dataset, num_candidates):\n",
        "  datasets = defaultdict(list)\n",
        "  for dialog in dataset:\n",
        "    persona = dialog[\"personality\"].copy()\n",
        "    for _ in range(args.personality_permutations):\n",
        "      for utterance in dialog[\"utterances\"]:\n",
        "          history = utterance[\"history\"][-(2 * args.max_history + 1) :]\n",
        "          for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "              lm_labels = bool(j == num_candidates - 1)\n",
        "              instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
        "              for input_name, input_array in instance.items():\n",
        "                  datasets[input_name].append(input_array)\n",
        "          datasets[\"mc_labels\"].append(num_candidates - 1)\n",
        "          datasets[\"n_candidates\"] = num_candidates\n",
        "      # permuted personalities\n",
        "      persona = [persona[-1]] + persona[:-1]\n",
        "  return datasets\n",
        "\n",
        "def top_filtering(logits, top_k=0.0, top_p=0.9, threshold=-float(\"Inf\"), filter_value=-float(\"Inf\")):\n",
        "  top_k = min(top_k, logits.size(-1))\n",
        "  if top_k > 0:\n",
        "      # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "      indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "      logits[indices_to_remove] = filter_value\n",
        "\n",
        "  if top_p > 0.0:\n",
        "      # Compute cumulative probabilities of sorted tokens\n",
        "      sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "      cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "      # Remove tokens with cumulative probability above the threshold\n",
        "      sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "      # Shift the indices to the right to keep also the first token above the threshold\n",
        "      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "      sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "      # Back to unsorted indices and set them to -infinity\n",
        "      indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "      logits[indices_to_remove] = filter_value\n",
        "\n",
        "  indices_to_remove = logits < threshold\n",
        "  logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return logits\n",
        "\n",
        "def generate_sequence(personality, history, tokenizer, model, current_output=None):\n",
        "  with torch.no_grad():\n",
        "    with amp.autocast():\n",
        "      special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "      if current_output is None:\n",
        "          current_output = []\n",
        "\n",
        "      # Args\n",
        "      max_length = 20\n",
        "      temperature = 0.7\n",
        "      top_k = 0\n",
        "      top_p = 0.9\n",
        "      do_sample = True\n",
        "      min_length = 1\n",
        "\n",
        "      for i in range(max_length):\n",
        "          instance = build_input_from_segments(\n",
        "              personality, history, current_output, tokenizer, with_eos=False\n",
        "          )\n",
        "\n",
        "          input_ids = torch.tensor(instance[\"input_ids\"], device=device).unsqueeze(0)\n",
        "          token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=device).unsqueeze(0)\n",
        "\n",
        "          logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "          logits = logits[0]\n",
        "\n",
        "          logits = logits[0, -1, :] / temperature\n",
        "          logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "          prev = torch.topk(probs, 1)[1] if not do_sample else torch.multinomial(probs, 1)\n",
        "          if i < min_length and prev.item() in special_tokens_ids:\n",
        "              while prev.item() in special_tokens_ids:\n",
        "                  if probs.max().item() == 1:\n",
        "                      break  # avoid infinite loop\n",
        "                  prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "          if prev.item() in special_tokens_ids:\n",
        "              break\n",
        "          current_output.append(prev.item())\n",
        "\n",
        "  return current_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSGs1lApHc_P"
      },
      "source": [
        "#### Without finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2pn43tpHfMS",
        "outputId": "f9f5310c-a921-422a-f506-bd6f828e05c1"
      },
      "source": [
        "# Personality\n",
        "test_personality=[\n",
        "  'I am Emma',\n",
        "  'I am a Dog',\n",
        "  'My gender is Female',\n",
        "  'My weight is 53.0',\n",
        "  'I was born on 2009',\n",
        "  'I am 11 years old',\n",
        "  'My breed is Retriever, Yellow Labrador',\n",
        "  'My color is White/Yello',\n",
        "  'I am house trained','i like to play with toys']\n",
        "\n",
        "# History\n",
        "test_history = [\n",
        "    \"Hi\",\n",
        "    \"woof woof\"\n",
        "]\n",
        "# New chat message\n",
        "test_message = \"what do you like to play with?\"\n",
        "\n",
        "print(test_personality)\n",
        "print(test_history)\n",
        "print(test_message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am Emma', 'I am a Dog', 'My gender is Female', 'My weight is 53.0', 'I was born on 2009', 'I am 11 years old', 'My breed is Retriever, Yellow Labrador', 'My color is White/Yello', 'I am house trained', 'i like to play with toys']\n",
            "['Hi', 'woof woof']\n",
            "what do you like to play with?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKbH7n3BHhdk",
        "outputId": "3d2c8573-aec9-4cf8-bbf6-2bcf948b6966"
      },
      "source": [
        "# Tokenize\n",
        "personality = [tokenizer.encode(s.lower()) for s in test_personality]\n",
        "history = [tokenizer.encode(s) for s in test_history]\n",
        "history.append(tokenizer.encode(test_message))\n",
        "# Generate output\n",
        "output = generate_sequence(personality, history, tokenizer, model)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(output, skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated text:\n",
            "i like to play with my dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7505DPQnKp4"
      },
      "source": [
        "#### With Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NML4bJfqqsQB",
        "outputId": "6b7732c2-a6e3-4367-b5a3-181cf636390f"
      },
      "source": [
        "# Setup Arguments\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n",
        "parser.add_argument(\"--validation_batch_size\", type=int, default=4, help=\"Batch size for validation\")\n",
        "parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"Number of candidates for training\")\n",
        "parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous exchanges to keep in history\")\n",
        "parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"Number of permutations of personality sentences\")\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Accumulate gradients on several steps\")\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=4e-05, help=\"Learning rate\")\n",
        "parser.add_argument(\"--lm_coef\", type=float, default=2.0, help=\"LM loss coefficient\")\n",
        "parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"Multiple-choice loss coefficient\")\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Optimizer weight decay\")\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=0, help=\"Number of warmup steps\")\n",
        "parser.add_argument(\"--warmup_ratio\", type=float, default=0.06, help=\"Warmup ratio\")\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Adam optimizer epsilon\")\n",
        "parser.add_argument(\"--verbose\", type=int, default=1, help=\"Verbose logging\")\n",
        "parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n",
        "parser.add_argument(\"--model_dir\", type=str, default=\"model_outputs\", help=\"Path to save model\")\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "logger.info(\"Arguments: %s\", args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:.:Arguments: Namespace(adam_epsilon=1e-08, epochs=1, gradient_accumulation_steps=1, learning_rate=4e-05, lm_coef=2.0, max_history=2, max_norm=1.0, mc_coef=1.0, model_dir='model_outputs', num_candidates=2, personality_permutations=1, train_batch_size=4, validation_batch_size=4, verbose=1, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEnqi_GnQYk"
      },
      "source": [
        "# If you want to try to fine tune from GPT2 pretrained weights directly here is the code\n",
        "# # Model\n",
        "# model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# # Tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# # Add special tokens to the tokenizer and model\n",
        "# orig_num_tokens = len(tokenizer.encoder)\n",
        "# # Add special tokens\n",
        "# num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "# if num_added_tokens > 0:\n",
        "#   model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "\n",
        "# # Convert model parameter tensors to CUDA tensors\n",
        "# model.to(device)\n",
        "\n",
        "# print(\"model type:\",type(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIdI0gwmy5d"
      },
      "source": [
        "#### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSOo8uGn4mi"
      },
      "source": [
        "# Read the personachat json file\n",
        "personachat_file = os.path.join(\"datasets\",\"personadogchat03.json\")\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "  personachat = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "4e1975ffa8e549e8bbe8fafd8cbd3719",
            "a6d5588e89ec4ab98a6a92b9194dd265",
            "dcaa9c5b9c104db2a73065da5030a618",
            "9b6f2260b1d041588d990aff5f1dcfb3",
            "ec2446f0932b430fae819100705633c4",
            "d02a16668b6948479211fb0ebab84a09",
            "f42ed2d2312c478781cfc3fbb16dd175",
            "d7059b2884704a1a834e1a5ed0e8fe09"
          ]
        },
        "id": "uiDdH5QTm1c9",
        "outputId": "0a671973-0dfa-4946-fce0-ec11fdfd5923"
      },
      "source": [
        "# Tokenize dataset\n",
        "train_processed = tokenize(personachat)\n",
        "\n",
        "print(\"train count:\",len(train_processed))\n",
        "print(train_processed[:2])\n",
        "\n",
        "train_num_candidates = len(train_processed[0][\"utterances\"][0][\"candidates\"])\n",
        "if args.num_candidates > 0:\n",
        "  train_num_candidates = min(args.num_candidates, train_num_candidates)\n",
        "\n",
        "# Prepare dataset inputs & outputs\n",
        "train_processed = prepare_datasets(train_processed, train_num_candidates)\n",
        "print(\"After adding inputs/outputs:\")\n",
        "print(\"train_processed keys:\", train_processed.keys())\n",
        "print(\"input_ids:\",len(train_processed[\"input_ids\"][0]),train_processed[\"input_ids\"][0])\n",
        "print(\"token_type_ids:\",len(train_processed[\"token_type_ids\"][0]),train_processed[\"token_type_ids\"][0])\n",
        "print(\"mc_token_ids:\",len(train_processed[\"mc_token_ids\"]))\n",
        "print(\"lm_labels:\",len(train_processed[\"lm_labels\"][0]),train_processed[\"lm_labels\"][0])\n",
        "print(\"mc_labels:\",len(train_processed[\"mc_labels\"]))\n",
        "print(\"n_candidates:\",train_processed[\"n_candidates\"])\n",
        "\n",
        "# Pad datasets\n",
        "train_processed = pad_dataset(train_processed, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "print(\"After Padding:\")\n",
        "print(\"input_ids:\",len(train_processed[\"input_ids\"][0]),train_processed[\"input_ids\"][0])\n",
        "print(\"token_type_ids:\",len(train_processed[\"token_type_ids\"][0]),train_processed[\"token_type_ids\"][0])\n",
        "print(\"mc_token_ids:\",len(train_processed[\"mc_token_ids\"]))\n",
        "print(\"lm_labels:\",len(train_processed[\"lm_labels\"][0]),train_processed[\"lm_labels\"][0])\n",
        "print(\"mc_labels:\",len(train_processed[\"mc_labels\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e1975ffa8e549e8bbe8fafd8cbd3719",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=807.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train count: 807\n",
            "[{'personality': [[40, 716, 21714, 18971], [40, 716, 257, 8532], [3666, 5279, 318, 15396], [3666, 3463, 318, 5996, 13, 15], [40, 373, 4642, 319, 3717, 2931, 1314], [3666, 15939, 318, 4990, 380, 964, 11, 12550, 45246], [3666, 3124, 318, 2635, 14, 14202], [40, 716, 2156, 8776]], 'utterances': [{'candidates': [[1662, 1107, 1312, 588, 852, 1363, 1804, 2147, 837, 340, 318, 7427, 5145], [31373, 612, 837, 1545, 5145, 644, 389, 345, 510, 284, 428, 845, 3734, 1110, 5633], [1014, 3608, 764, 1312, 2513, 319, 262, 10481, 290, 2342, 262, 26428, 790, 1755, 764], [40909, 837, 484, 389, 9616, 616, 11077, 290, 1312, 3613, 1637, 981, 287, 4152], [72, 1101, 7926, 837, 1312, 1101, 407, 5385, 351, 607, 764, 1312, 2883, 3555], [258, 318, 5650, 837, 523, 339, 7622, 502, 845, 922, 1664, 611, 345, 651, 644, 1312, 1612], [72, 1842, 1642, 649, 8242, 503, 286, 1468, 290, 4379, 616, 3988, 287, 1398, 1657, 510], [3810, 318, 991, 379, 262, 2479, 339, 655, 7832, 284, 711, 319, 616, 220, 13323, 505, 477, 1110], [8505, 1312, 716, 6405, 764, 1312, 423, 587, 329, 718, 812, 764, 1312, 1842, 852, 6405, 764], [8505, 340, 318, 644, 318, 326, 345, 466, 5633], [31373, 837, 703, 389, 345, 1909, 5633], [10919, 466, 345, 2050, 290, 3197, 5633, 1312, 423, 362, 14850, 326, 2050, 783, 764], [5303, 764, 644, 466, 345, 466, 329, 534, 1693, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0]], 'history': [[5303, 837, 703, 389, 345, 5633]]}, {'candidates': [[25991, 393, 3863, 826, 287, 616, 20994], [31373, 837, 703, 389, 345, 1804, 1909, 5633], [72, 2107, 287, 257, 1310, 2156, 284, 3613], [20342, 612, 922, 6672, 703, 389, 345, 5633], [36590, 421, 25678, 764, 1312, 1842, 340, 5145, 663, 1392, 617, 4077, 287, 340, 764], [5562, 338, 1049, 764, 1097, 2476, 1842, 1165, 764, 1312, 670, 355, 257, 13028, 11915, 764], [1219, 1107, 5633, 1560, 502, 517, 764], [72, 2630, 281, 2708, 2691, 546, 703, 4813, 389, 46442, 959, 1231, 16029, 764], [72, 716, 1049, 764, 1149, 14485, 290, 1312, 389, 655, 5586, 994, 4964, 262, 2344, 6611, 764], [5562, 318, 523, 1049, 764, 810, 466, 345, 2107, 5633], [11873, 326, 1312, 836, 470, 2107, 351, 2687, 1312, 714, 4414, 422, 257, 4273, 764], [1820, 1438, 318, 21714, 18971]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633]]}, {'candidates': [[43669, 837, 616, 4706, 285, 40302, 318, 257, 1049, 1048, 764, 644, 2073, 466, 345, 466, 5633], [5562, 318, 991, 257, 1049, 5032, 284, 423, 764], [72, 1053, 1100, 257, 1178, 416, 4628, 14863, 1976, 15386, 764, 1312, 716, 257, 21221, 764, 345, 5633], [1219, 12876, 5145, 787, 2565, 284, 502, 5145], [10919, 466, 345, 466, 329, 670, 5633, 1312, 1101, 257, 1597, 4870], [72, 1100, 523, 1312, 836, 470, 760, 546, 326, 764, 290, 1312, 1053, 1293, 385, 287, 616, 33515, 764, 345, 5633], [72, 1842, 2647, 837, 2592, 21443, 2647], [72, 1101, 1654, 340, 857, 1312, 4499, 340, 7199, 812, 2084, 290, 1312, 991, 3357, 764], [993, 837, 2642, 5145, 4171, 318, 616, 2090, 3124, 764, 644, 45578, 466, 345, 423, 5633], [72, 716, 3478, 764, 1312, 588, 2523, 546, 6844, 290, 11875, 764, 883, 389, 616, 4004, 4695, 764], [72, 1239, 423, 284, 2822, 36541, 780, 1312, 787, 340, 764, 326, 5238, 588, 1257, 764], [72, 716, 257, 8532]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633]]}, {'candidates': [[31373, 837, 703, 389, 345, 1909, 5633], [4919, 867, 6844, 466, 345, 423, 5633], [72, 711, 275, 32735, 351, 616, 5229], [10919, 466, 345, 588, 284, 1663, 287, 340, 5633], [72, 2107, 351, 616, 3290, 1312, 716, 1642, 3037, 329, 683, 284, 2740], [72, 716, 257, 13430, 379, 336, 272, 3841, 764, 318, 534, 1995, 257, 6240, 287, 262, 514, 5633], [72, 716, 15396]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633]]}, {'candidates': [[72, 1842, 6508, 37263, 837, 3360, 1312, 4483, 655, 257, 1178, 1165, 867, 42254], [31373, 703, 389, 345, 9975], [72, 2107, 287, 2386, 361, 3317, 340, 318, 1464, 3024, 994, 764], [270, 318, 3621, 3360, 284, 3368, 262, 15779, 290, 1312, 460, 6128, 287, 616, 279, 1228, 17485, 19462, 764], [10378, 2412, 319, 262, 1285, 1312, 2652, 510, 2739, 257, 1256, 286, 670], [5303, 1312, 1101, 264, 453, 837, 1312, 2107, 351, 616, 6029, 6844, 287, 256, 7495, 837, 649, 502, 87, 3713, 764], [78, 880, 345, 815, 787, 640, 329, 534, 2116, 284, 423, 1257], [3810, 318, 4077, 5145, 644, 1611, 286, 2647, 466, 345, 588, 5633, 1312, 1842, 3881, 5145, 32497, 389, 7427, 5145], [271, 340, 588, 3971, 563, 1787, 353], [8505, 764, 389, 345, 1804, 1997, 3499, 1909, 5633], [454, 3797, 38609, 7558, 837, 673, 468, 884, 257, 2089, 7947, 329, 1692, 2057], [72, 1842, 45002, 3024, 6844, 837, 644, 466, 345, 588, 284, 4483], [10919, 318, 534, 4004, 1517, 284, 466, 287, 534, 13952, 640], [1820, 3463, 318, 5996, 13, 15]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633]]}, {'candidates': [[71, 6532, 1787, 353, 837, 616, 1115, 3988, 2314, 651, 1576, 286, 340, 5145], [4871, 605, 837, 21274, 837, 15504, 837, 257, 1310, 1499, 290, 616, 9397, 2111, 2222, 502, 1459, 351, 4095], [43669, 837, 1312, 1053, 587, 8914, 329, 257, 649, 1752, 1201, 1312, 5710, 503, 286, 4152, 764], [5303, 837, 703, 389, 345, 1804, 1909, 5633], [707, 5927, 5145, 6541, 290, 42152, 318, 616, 4004, 764], [27485, 5633, 427, 6238, 644, 345, 8066, 466, 5633], [10919, 2057, 466, 345, 588, 837, 611, 407, 8701, 8873, 11799, 5633], [72, 460, 691, 5967, 1312, 716, 257, 691, 1200], [72, 1107, 716, 1165, 764, 1049, 3375, 284, 345, 1165, 764], [4598, 345, 423, 597, 4004, 5701, 5633], [270, 318, 1016, 2495, 922, 764, 12431, 5633], [24494, 1312, 588, 284, 16853, 3360, 764, 345, 711, 281, 8875, 5633], [29762, 477, 1088, 262, 3096, 883, 1312, 4545, 389, 616, 3988, 764], [72, 1842, 5916, 837, 422, 24535, 284, 38883], [72, 588, 2647, 466, 345, 5633], [8807, 1243, 15847, 287, 616, 898, 9592], [72, 373, 4642, 319, 3717, 2931, 1314]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 5996, 13, 15], [12518, 547, 345, 4642, 5633]]}, {'candidates': [[31642, 837, 523, 484, 714, 466, 616, 1693, 329, 502, 837, 1312, 716, 257, 44511, 764, 644, 466, 345, 5633], [72, 670, 379, 3491, 18999, 7351, 6891, 284, 661], [5303, 1312, 716, 1804, 880, 703, 389, 345], [75, 5309, 345, 423, 534, 1995, 764, 1312, 3551, 287, 616, 3989, 790, 1110, 764], [72, 635, 670, 287, 262, 6308, 2831, 379, 262, 1957, 5011, 3650, 764], [270, 318, 764, 616, 1995, 373, 257, 15849, 996, 523, 673, 1392, 502, 287, 262, 2831], [3003, 466, 345, 670, 379], [1820, 15939, 318, 4990, 380, 964, 11, 12550, 45246]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 5996, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 3717, 2931, 1314], [10919, 15939, 389, 345, 5633]]}, {'candidates': [[1820, 30773, 1842, 607, 1165, 764, 2506, 815, 764], [72, 1101, 407, 12776, 453, 1964, 764, 1312, 1053, 1913, 5009, 996, 764], [8505, 837, 326, 338, 1521, 1312, 1949, 284, 3613, 355, 881, 1637, 355, 1312, 460], [4598, 345, 7523, 393, 4144, 5633], [74, 2442, 479, 1806, 293, 257, 33826, 5356, 19751, 837, 1201, 616, 10955, 318, 319, 33826, 5356, 5145, 345, 5633], [3003, 466, 345, 670, 2318, 12590, 5633], [5562, 5238, 2495, 764, 466, 345, 588, 262, 32728, 31804, 5633], [72, 561, 407, 760, 837, 1312, 1053, 1239, 587, 503, 286, 262, 514, 764], [31373, 837, 1312, 716, 3734, 837, 466, 345, 423, 597, 45578, 5633], [69, 3289, 1312, 13537, 852, 257, 4255, 1816, 284, 1524, 764], [5303, 837, 703, 389, 345, 5633, 466, 345, 423, 597, 17252, 5633], [1820, 3124, 318, 2635, 14, 14202]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 5996, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 3717, 2931, 1314], [10919, 15939, 389, 345, 5633], [1820, 15939, 318, 4990, 380, 964, 11, 12550, 45246], [10919, 3124, 389, 345, 5633]]}, {'candidates': [[568, 1312, 655, 50052, 616, 4190, 32749, 290, 1807, 1312, 561, 8537, 257, 1643], [24494, 764, 1312, 9067, 12027, 3812, 10010, 256, 20858, 3589, 764], [4053, 837, 1312, 635, 1100, 3835, 764, 1365, 621, 6918, 3360, 764], [482, 1312, 1101, 9675, 534, 3734, 1312, 1101, 635, 880, 764, 466, 1997, 329, 1257, 5633], [5562, 338, 6639, 345, 1683, 9396, 5633], [2339, 16667, 20382, 319, 17564, 1597, 764], [20342, 1312, 716, 1804, 1049, 764, 14256, 5238, 922, 1312, 1107, 588, 38883, 5916, 20698], [270, 318, 764, 616, 1995, 373, 257, 15849, 996, 523, 673, 1392, 502, 287, 262, 2831], [41599, 1312, 466, 407, 651, 326, 764, 19462, 764], [72, 2107, 287, 20518, 1122, 475, 373, 635, 287, 285, 560, 1044, 329, 27416, 1016, 284, 1524], [72, 3088, 340, 1752, 837, 475, 750, 407, 2883, 340, 764], [49459, 351, 257, 922, 4405, 1107, 475, 257, 1256, 286, 9280, 2647], [47288, 691, 1141, 262, 572, 1622, 764, 466, 345, 588, 9015, 5633], [8505, 1312, 716, 2156, 8776]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 21714, 18971], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 5996, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 3717, 2931, 1314], [10919, 15939, 389, 345, 5633], [1820, 15939, 318, 4990, 380, 964, 11, 12550, 45246], [10919, 3124, 389, 345, 5633], [1820, 3124, 318, 2635, 14, 14202], [533, 345, 2156, 8776, 5633]]}]}, {'personality': [[40, 716, 38058, 268], [40, 716, 257, 8532], [3666, 5279, 318, 15396], [3666, 3463, 318, 3933, 13, 15], [40, 373, 4642, 319, 1584, 2931, 1314], [3666, 15939, 318, 5181, 993, 2852, 64, 49072, 8532], [3666, 3124, 318, 2635, 14, 9915], [40, 716, 2156, 8776]], 'utterances': [{'candidates': [[72, 588, 14032, 1165, 764, 3223, 14032, 837, 407, 21606, 2194], [41599, 284, 3285, 326, 764, 345, 714, 1949, 1762, 2691, 764, 2499, 329, 502], [72, 588, 284, 6594, 616, 7161, 284, 1598, 616, 1182, 837, 393, 772, 48342, 764], [1219, 329, 1654, 764, 326, 338, 257, 1109, 5145], [732, 550, 257, 2910, 8824, 764, 290, 1312, 373, 4642, 2354, 739, 340, 764], [3003, 389, 345, 422, 5633, 1312, 1101, 287, 2386, 72], [72, 588, 262, 38283, 1165, 764, 616, 2802, 16334, 300, 521, 4397, 1752, 736, 287, 8309, 764], [77, 3008, 475, 340, 318, 287, 257, 1256, 286, 616, 3971, 563, 1787, 353, 3835], [1169, 6403, 286, 497, 17088, 300, 11690, 837, 663, 257, 1049, 1524, 764], [8505, 340, 318, 257, 7072, 764, 644, 466, 345, 466, 379, 1363, 5633], [2958, 866, 284, 36023, 289, 1436, 292, 290, 2883, 262, 1660, 351, 674, 1641, 5145], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0]], 'history': [[5303, 837, 703, 389, 345, 5633]]}, {'candidates': [[14150, 345, 1683, 31678, 276, 329, 1997, 5633, 1312, 750, 329, 10801, 1752], [42773, 837, 326, 264, 7427, 5145, 287, 1254, 351, 345, 764], [72, 765, 284, 2107, 8097, 1037, 502, 4451, 88], [20342, 612, 837, 1312, 716, 1804, 880, 837, 345, 5633], [896, 23036, 466, 345, 5806, 15232, 5633], [8807, 319, 262, 21511], [72, 481, 787, 883, 6844, 804, 45308, 764, 287, 262, 6672, 996, 837, 329, 1312, 3993, 2739, 764], [8505, 837, 1312, 4236, 764, 290, 356, 423, 587, 3047, 616, 734, 9230, 11903, 355, 2139, 6844, 764], [258, 655, 857, 407, 588, 616, 15232], [482, 33847], [29762, 477, 1088, 262, 3096, 883, 1312, 4545, 389, 616, 3988, 764], [1820, 1438, 318, 38058, 268]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633]]}, {'candidates': [[72, 1842, 340, 355, 881, 355, 1312, 1842, 6155, 6844, 764, 810, 466, 345, 670, 5633], [5303, 2147, 881, 655, 1392, 1760, 2712, 616, 10047], [1169, 4979, 10182, 502, 14380], [4919, 389, 345, 1804, 1909, 5633], [31642, 764, 880, 764, 326, 290, 6918, 764, 1312, 7558, 423, 30070, 1016, 287, 262, 4469], [1891, 618, 1312, 373, 7351, 11292, 484, 2826, 257, 10010, 329, 262, 39368], [8807, 262, 530, 1312, 1364, 379, 262, 7779, 2245, 764, 345, 5633], [72, 1690, 4320, 286, 11300, 284, 1011, 5986, 287, 262, 6290], [72, 716, 257, 8532]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633]]}, {'candidates': [[5303, 703, 389, 345, 1804], [47288, 1312, 12692, 616, 9686, 618, 23751, 78, 29959, 4378, 293, 340], [5303, 703, 389, 345, 1804], [39532, 290, 474, 6996, 837, 24926, 5633, 326, 373, 616, 4004, 16251, 18545, 355, 257, 5141, 764], [43669, 837, 475, 340, 1107, 7622, 502, 22804, 764, 635, 837, 340, 7622, 502, 19695, 764], [3919, 764, 1312, 655, 2342, 31557, 764, 262, 5770, 11358, 837, 4632, 764], [439, 14532, 7263, 764, 340, 1107, 318, 764, 611, 345, 1502, 772, 1752, 257, 1227, 837, 340, 318, 2861, 340, 764], [39054, 5633, 326, 1165, 287, 7586, 444, 5633, 1312, 716, 14376, 27089, 5145], [24494, 764, 262, 691, 1295, 1312, 467, 318, 284, 257, 13546, 523, 1312, 460, 5916, 764], [72, 716, 15396]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633]]}, {'candidates': [[5832, 423, 284, 1282, 284, 299, 66, 290, 1949, 617, 275, 65, 80, 764], [83, 14992, 284, 1394, 5814, 663, 587, 43079, 477, 1110, 764], [24494, 3608, 290, 326, 264, 523, 1049], [71, 50107, 318, 616, 4004, 764, 290, 663, 523, 2582, 5145], [72, 1101, 1654, 340, 373, 764, 1312, 2883, 5059, 616, 1097, 764], [72, 588, 33826, 666, 7914, 9280, 2647, 837, 345, 5633], [44460, 837, 1312, 35695, 345, 319, 534, 45712, 590], [5562, 5238, 588, 1257, 764, 1312, 655, 8155, 616, 717, 1097, 5145], [10919, 1611, 286, 11016, 466, 345, 670, 329, 5633], [72, 731, 340, 857, 5145, 1312, 1183, 423, 284, 1502, 617, 422, 345, 764], [9930, 423, 4697, 3466, 764, 17207, 837, 345, 761, 257, 1693, 5633], [270, 318, 8788, 764, 616, 1995, 1595, 470, 772, 1842, 502], [1169, 3600, 373, 616, 1266, 1545, 287, 4152], [72, 670, 379, 262, 331, 76, 6888, 290, 1312, 1101, 257, 2888, 1165, 764], [1820, 3463, 318, 3933, 13, 15]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633]]}, {'candidates': [[72, 892, 477, 3397, 1254, 326, 835, 379, 530, 966, 764], [72, 423, 718, 6844, 837, 477, 8197, 764], [4919, 389, 345, 1804, 319, 428, 18259, 1110, 5633], [43669, 3360, 764, 1312, 1101, 1016, 284, 1282, 503, 286, 262, 21615, 530, 1110, 764], [22850, 810, 466, 345, 2107, 5633], [31373, 612, 837, 1545, 5145, 644, 389, 345, 510, 284, 428, 845, 3734, 1110, 5633], [1662, 477, 477, 764, 2506, 468, 511, 898, 15387, 764, 1312, 1612, 1312, 1100, 4336, 10165, 764], [3810, 318, 991, 379, 262, 2479, 339, 655, 7832, 284, 711, 319, 616, 220, 13323, 505, 477, 1110], [4919, 389, 345, 24976, 340, 5633], [896, 257, 922, 1517, 1312, 3888, 845, 1290, 422, 616, 1363, 290, 2180, 336, 20949, 764], [1219, 703, 3608, 5145, 262, 14260, 389, 18857, 764, 1312, 1842, 4695, 764, 1312, 1053, 1775, 262, 1396, 353, 44915, 5145], [5562, 338, 1049, 329, 606, 42254], [72, 373, 4642, 319, 1584, 2931, 1314]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 3933, 13, 15], [12518, 547, 345, 4642, 5633]]}, {'candidates': [[25159, 286, 289, 633, 22242, 1312, 423, 513, 802, 364], [72, 423, 587, 257, 38581, 459, 329, 513, 812, 663, 1107, 1257], [5562, 338, 7427, 837, 1312, 1549, 1842, 326, 764], [72, 4545, 1029, 1524, 290, 466, 617, 1242, 6097], [72, 1101, 422, 477, 625, 764, 1312, 1053, 9422, 21732, 422, 6041, 286, 1180, 4266, 764], [1640, 1654, 837, 32596, 3160, 290, 1708, 534, 7506, 318, 1593, 1312, 892, 764, 764, 764], [1820, 15939, 318, 5181, 993, 2852, 64, 49072, 8532]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 3933, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 1584, 2931, 1314], [10919, 15939, 389, 345, 5633]]}, {'candidates': [[568, 466, 345, 4545, 534, 3988, 5633, 936, 287, 262, 1398, 2119, 5633], [3919, 17252, 1865, 764, 616, 3656, 16887, 607, 640, 7219, 351, 674, 1751], [2763, 290, 9891, 837, 703, 546, 345], [732, 423, 257, 8598, 2837, 503, 994, 1444, 285, 5067, 64, 290, 1312, 467, 612, 4632, 764], [293, 1032, 655, 25760, 523, 922, 996, 5145, 466, 345, 22647, 393, 19984, 5633], [43669, 484, 389, 477, 826, 1312, 423, 407, 587, 284, 257, 1256], [72, 1842, 4964, 9283, 24349, 319, 27737, 1528, 764], [27547, 5145, 30060, 318, 257, 4004, 25629, 286, 6164, 764, 523, 837, 45038, 534, 3348, 319, 5633], [72, 466, 837, 1312, 670, 351, 281, 5044, 9992, 764, 1312, 2883, 1762, 351, 4695, 764], [72, 1842, 3555, 290, 8680, 284, 1499, 2647], [1820, 3124, 318, 2635, 14, 9915]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 3933, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 1584, 2931, 1314], [10919, 15939, 389, 345, 5633], [1820, 15939, 318, 5181, 993, 2852, 64, 49072, 8532], [10919, 3124, 389, 345, 5633]]}, {'candidates': [[5562, 318, 257, 922, 835, 284, 1234, 340, 764], [37035, 340, 764, 2592, 284, 4695, 764, 466, 345, 423, 17252, 5633], [5562, 318, 257, 10195, 837, 616, 3367, 474, 1047, 318, 513, 837, 339, 338, 31856, 284, 6844, 764], [72, 6004, 284, 262, 5243, 287, 616, 7779, 764, 4632, 1499, 764, 616, 3290, 7832, 307, 2788, 16206, 764, 257, 1310, 9707, 764], [72, 550, 284, 3708, 257, 890, 640, 284, 651, 606, 1165, 764], [72, 1239, 8288, 595, 1681, 764, 1312, 466, 588, 3067, 996, 764], [14150, 345, 1683, 2982, 286, 262, 4097, 262, 25152, 5355, 764], [31373, 764, 663, 20050, 764, 703, 318, 534, 5041, 1016, 5633], [72, 716, 422, 3124, 4533, 764, 1842, 340, 994, 287, 3931, 329, 24522, 663, 616, 4004], [10919, 750, 345, 466, 284, 651, 326, 1438, 5633], [261, 616, 5296, 284, 340, 3400], [8505, 1312, 716, 2156, 8776]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [1820, 1438, 318, 38058, 268], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [72, 716, 15396], [10919, 318, 534, 3463, 5633], [1820, 3463, 318, 3933, 13, 15], [12518, 547, 345, 4642, 5633], [72, 373, 4642, 319, 1584, 2931, 1314], [10919, 15939, 389, 345, 5633], [1820, 15939, 318, 5181, 993, 2852, 64, 49072, 8532], [10919, 3124, 389, 345, 5633], [1820, 3124, 318, 2635, 14, 9915], [533, 345, 2156, 8776, 5633]]}]}]\n",
            "After adding inputs/outputs:\n",
            "train_processed keys: dict_keys(['input_ids', 'token_type_ids', 'mc_token_ids', 'lm_labels', 'mc_labels', 'n_candidates'])\n",
            "input_ids: 64 [50257, 40, 716, 21714, 18971, 40, 716, 257, 8532, 3666, 5279, 318, 15396, 3666, 3463, 318, 5996, 13, 15, 40, 373, 4642, 319, 3717, 2931, 1314, 3666, 15939, 318, 4990, 380, 964, 11, 12550, 45246, 3666, 3124, 318, 2635, 14, 14202, 40, 716, 2156, 8776, 50261, 5303, 837, 703, 389, 345, 5633, 50260, 5303, 764, 644, 466, 345, 466, 329, 534, 1693, 5633, 50258]\n",
            "token_type_ids: 64 [50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260]\n",
            "mc_token_ids: 14526\n",
            "lm_labels: 64 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "mc_labels: 7263\n",
            "n_candidates: 2\n",
            "After Padding:\n",
            "input_ids: 125 [50257, 40, 716, 21714, 18971, 40, 716, 257, 8532, 3666, 5279, 318, 15396, 3666, 3463, 318, 5996, 13, 15, 40, 373, 4642, 319, 3717, 2931, 1314, 3666, 15939, 318, 4990, 380, 964, 11, 12550, 45246, 3666, 3124, 318, 2635, 14, 14202, 40, 716, 2156, 8776, 50261, 5303, 837, 703, 389, 345, 5633, 50260, 5303, 764, 644, 466, 345, 466, 329, 534, 1693, 5633, 50258, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]\n",
            "token_type_ids: 125 [50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]\n",
            "mc_token_ids: 14526\n",
            "lm_labels: 125 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "mc_labels: 7263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP0nK-ZIq83e",
        "outputId": "dc27f404-5e17-414f-f60f-531f278d3021"
      },
      "source": [
        "# Create Tensors\n",
        "train_tensor_datasets = []\n",
        "validate_tensor_datasets = []\n",
        "for input_name in MODEL_INPUTS:\n",
        "  train_tensor = torch.tensor(train_processed[input_name])\n",
        "  if input_name != \"mc_labels\":\n",
        "      train_tensor = train_tensor.view((-1, train_processed[\"n_candidates\"]) + train_tensor.shape[1:])\n",
        "  train_tensor_datasets.append(train_tensor)\n",
        "\n",
        "# Tensor Dataset\n",
        "train_tensor_dataset = TensorDataset(*train_tensor_datasets)\n",
        "\n",
        "# Create Data Loaders\n",
        "train_data_sampler = RandomSampler(train_tensor_dataset)\n",
        "train_data_loader = DataLoader(train_tensor_dataset, sampler=train_data_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "logger.info(\"Train DataLoader (Batch, Candidates, Seq length): {}\".format(train_tensor_dataset.tensors[0].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:.:Train DataLoader (Batch, Candidates, Seq length): torch.Size([7263, 2, 125])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSHgSB0rB4Q"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__vhSqnZrDOY",
        "outputId": "d6a93d8a-1efb-487e-c48e-f088209d5560"
      },
      "source": [
        "training_steps = len(train_data_loader) // args.gradient_accumulation_steps * args.epochs\n",
        "\n",
        "warmup_steps = math.ceil(training_steps * args.warmup_ratio)\n",
        "warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
        "print(\"warmup_steps:\", warmup_steps)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=training_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "warmup_steps: 109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "7dc4c94626974c8c86f6ec7f9226ca72",
            "32461ab9bc0d4aaab7471eb68d3a1bc4",
            "66aeffb8a7a64448b56d809a0c713421",
            "ee7e7046f1914a6ab0f4a7fc81ecc61b",
            "618b03273e77458294c0851df3a5bd33",
            "1825156af1904e0390aaa006e9891737",
            "8ab9d7159b5d4fd0ba1996009f6cb4f0",
            "04e46c47507f4cfc9d574f9df22d66d7",
            "3ba4d5fc2c614525bce41a79190e2bc9",
            "c22bc7045dd84d77a33bd7310af1e02e",
            "4c84d74e78644e1db46af679c8e25261",
            "25f1a5b0d88346ceb3113f3b41cfc60c",
            "a1da0be38b784e0cb03e4a6215fa515a",
            "96e9d129d96743a58b211746598d140f",
            "c787f47fcf444a3ca10f52e5ccef37ef",
            "acfaa86d3e054f3488e29fd0ba38fdc7"
          ]
        },
        "id": "GetJDGLbrILv",
        "outputId": "b5074d9a-bdc1-49b7-b12f-6183b1940732"
      },
      "source": [
        "# Free Memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "disable = True if args.verbose == 0 else False\n",
        "global_step = 0\n",
        "training_progress_scores = None\n",
        "tr_loss, logging_loss = 0.0, 0.0\n",
        "model.zero_grad()\n",
        "train_iterator = trange(int(args.epochs), desc=\"Epoch\", disable=disable)\n",
        "epoch_number = 0\n",
        "best_eval_metric = None\n",
        "early_stopping_counter = 0\n",
        "logging_steps = 50\n",
        "\n",
        "# Create directory to save model\n",
        "model_dir = args.model_dir\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in train_iterator:\n",
        "    model.train()\n",
        "    train_iterator.set_description(f'Epoch {epoch_number + 1} of {args.epochs}')\n",
        "    batch_iterator = tqdm(\n",
        "        train_data_loader,\n",
        "        desc=f'Running Epoch {epoch_number} of {args.epochs}',\n",
        "        disable=disable,\n",
        "        mininterval=0,\n",
        "    )\n",
        "    for step, batch in enumerate(batch_iterator):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "\n",
        "        with amp.autocast():\n",
        "          model_outputs = model(\n",
        "              input_ids,\n",
        "              token_type_ids=token_type_ids,\n",
        "              mc_token_ids=mc_token_ids,\n",
        "              mc_labels=mc_labels,\n",
        "              labels=lm_labels,\n",
        "          )\n",
        "          mc_loss = model_outputs[\"mc_loss\"]\n",
        "          lm_loss = model_outputs[\"loss\"]\n",
        "          loss = lm_loss * args.lm_coef + mc_loss * args.mc_coef\n",
        "\n",
        "        current_loss = loss.item()\n",
        "\n",
        "        print(\"\\rRunning loss: %f\" % current_loss, end=\"\")\n",
        "\n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "          loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update learning rate schedule\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            if logging_steps > 0 and global_step % logging_steps == 0:\n",
        "                logging_loss = tr_loss\n",
        "\n",
        "    epoch_number += 1\n",
        "\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "logger.info(\"Execution time (mins): %s\",execution_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dc4c94626974c8c86f6ec7f9226ca72",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ba4d5fc2c614525bce41a79190e2bc9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=1816.0, style=ProgressStyle(de…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rRunning loss: 3.742670\rRunning loss: 6.735260"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.000042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:.:Execution time (mins): 5.8799940824508665\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\rRunning loss: 0.000052\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dOxj3ODrcn0"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gj_Pg2fwiLX",
        "outputId": "0b8ad966-59b8-4c14-e4e5-32c7c2e2fe27"
      },
      "source": [
        "# Personality\n",
        "test_personality=[\n",
        "  'I am Emma',\n",
        "  'I am a Dog',\n",
        "  'My gender is Female',\n",
        "  'My weight is 53.0',\n",
        "  'I was born on 2009',\n",
        "  'I am 11 years old',\n",
        "  'My breed is Retriever, Yellow Labrador',\n",
        "  'My color is White/Yello',\n",
        "  'I am house trained','i like to play with toys']\n",
        "\n",
        "# History\n",
        "test_history = [\n",
        "    \"Hi\",\n",
        "    \"woof woof\"\n",
        "]\n",
        "\n",
        "print(test_personality)\n",
        "print(test_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am Emma', 'I am a Dog', 'My gender is Female', 'My weight is 53.0', 'I was born on 2009', 'I am 1 years old', 'My breed is Retriever, Yellow Labrador', 'My color is White/Yello', 'I am house trained', 'i like to play with toys']\n",
            "['Hi', 'woof woof']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtQFVCptreN-",
        "outputId": "2cb91c18-fcdf-4ce8-ead5-7a2e963cb74e"
      },
      "source": [
        "# New chat message\n",
        "test_message = \"How old are you?\" # what do you like to play with?, Are you house trained?, How old are you?\n",
        "\n",
        "# Tokenize test inputs\n",
        "personality = [tokenizer.encode(s.lower()) for s in test_personality]\n",
        "history = [tokenizer.encode(s) for s in test_history]\n",
        "history.append(tokenizer.encode(test_message))\n",
        "# Generate output\n",
        "output = generate_sequence(personality, history, tokenizer, model)\n",
        "\n",
        "print(\"Question:\")\n",
        "print(test_message)\n",
        "\n",
        "print(\"Answer:\")\n",
        "print(tokenizer.decode(output, skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:\n",
            "How old are you?\n",
            "Answer:\n",
            "i am 1 year old\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhmimrMPYz5P"
      },
      "source": [
        "Now we see some promissing results 🐶🐶🐶👏👏👏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3eRTcQf9bT"
      },
      "source": [
        "#### Nano Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnKa96SqgAdF"
      },
      "source": [
        "##### Question 1\n",
        "\n",
        "* **What is the purpose of the two heads in the GPT2 Double Head Model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pgnHu_EgHPh"
      },
      "source": [
        "##### Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvAkob_IK7Ac"
      },
      "source": [
        "One head is the generate the languge and the other to check correctness of the generated answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Sb7iX4gDUG"
      },
      "source": [
        "##### Question 2\n",
        "\n",
        "* **What was the problem with finetuning our dogs dataset directly with GPT2 pretrained weights?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDdXTvcvgKw4"
      },
      "source": [
        "##### Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRLA-9_BLCeJ"
      },
      "source": [
        "Our dialog task requires a variety of dialog data. We dont have a large engough dataset for conversation with a dog. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpT2F77EgDvw"
      },
      "source": [
        "##### Question 3\n",
        "\n",
        "* **What are word, position, and segments embeddings in the above model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnz-qTygL4T"
      },
      "source": [
        "##### Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzlBqij0LemC"
      },
      "source": [
        "Word embeddings are where each word in the dataset is mapped to a numberical vector.\n",
        "\n",
        "Position embeddings give the model a sense of order we add a piece of information to each word about its position in the sentence\n",
        "\n",
        "Segment embeddings give the model an idea of what the persona, history, and answers are\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4otMF2F1_yRX"
      },
      "source": [
        "## **Save Model/Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdcU9ehZ_0T7",
        "outputId": "f2eccf62-8e48-48f1-e149-9a0842675d52"
      },
      "source": [
        "# Save\n",
        "model_dir = \"trained_model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trained_model/tokenizer_config.json',\n",
              " 'trained_model/special_tokens_map.json',\n",
              " 'trained_model/vocab.json',\n",
              " 'trained_model/merges.txt',\n",
              " 'trained_model/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fqx6Dz0_8JE",
        "outputId": "081c4c07-0f79-4336-9917-1e691a2778c7"
      },
      "source": [
        "!zip -r finetuned_model_epochs_1.zip trained_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: trained_model/ (stored 0%)\n",
            "  adding: trained_model/merges.txt (deflated 53%)\n",
            "  adding: trained_model/tokenizer_config.json (deflated 67%)\n",
            "  adding: trained_model/added_tokens.json (deflated 42%)\n",
            "  adding: trained_model/config.json (deflated 50%)\n",
            "  adding: trained_model/pytorch_model.bin (deflated 9%)\n",
            "  adding: trained_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: trained_model/vocab.json (deflated 63%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zAdsGp0jKn"
      },
      "source": [
        "## **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuzdfghU0jpm"
      },
      "source": [
        "### Research Papers\n",
        "* [Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)\n",
        "* [GPT-2 (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRdvY3Xl0tkb"
      },
      "source": [
        "### Code\n",
        "\n",
        "* [Building a State-of-the-Art Conversational AI with Transfer Learning](https://github.com/huggingface/transfer-learning-conv-ai)\n",
        "* [Summary of the models](https://huggingface.co/transformers/model_summary.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKphvNB61R-J"
      },
      "source": [
        "### Articles\n",
        "\n",
        "* [How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n",
        "* [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n",
        "* [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)"
      ]
    }
  ]
}